#
# Translators:
# Alfred <alfredhuang211@qq.com>, 2015
# zhaochao <zhaochao1984@gmail.com>, 2015
# 秋林 <chenqioulin_1983@163.com>, 2015
# 颜海峰 <yanheven@gmail.com>, 2014
msgid ""
msgstr ""
"Project-Id-Version: OpenStack Manuals\n"
"POT-Creation-Date: 2015-05-06 19:56+0000\n"
"PO-Revision-Date: 2015-05-06 19:56+0000\n"
"Last-Translator: openstackjenkins <jenkins@openstack.org>\n"
"Language-Team: Chinese (China) (http://www.transifex.com/projects/p/"
"openstack-manuals-i18n/language/zh_CN/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: zh_CN\n"
"Plural-Forms: nplurals=1; plural=0;\n"

msgid "Cloud controller cluster stack"
msgstr "OpenStack 控制服务 HA 集群配置"

msgid ""
"The cloud controller runs on the management network and must talk to all "
"other services."
msgstr ""
"OpenStack 控制服务运行在管理网络上，可以和其它任何 OpenStack 服务进行交互。"

msgid "OpenStack network nodes"
msgstr "OpenStack 网络节点服务器"

msgid "OpenStack network nodes contain:"
msgstr "OpenStack 网络节点运行以下服务："

msgid "Neutron DHCP agent"
msgstr "Neutron DHCP 代理服务"

msgid "Neutron L2 agent"
msgstr "Neutron L2 代理服务"

msgid "Neutron L3 agent"
msgstr "Neutron L3 代理服务"

msgid "Neutron metadata agent"
msgstr "Neutron 元数据代理服务"

msgid "Neutron LBaaS agent"
msgstr "Neutron LBaas 代理服务"

msgid ""
"The neutron L2 agent does not need to be highly available. It has to be "
"installed on each data forwarding node and controls the virtual networking "
"drivers as Open vSwitch or Linux Bridge. One L2 agent runs per node and "
"controls its virtual interfaces. That's why it cannot be distributed and "
"highly available."
msgstr ""
"Neutron L2 代理服务不需要实现高可用。在所有提供数据转发的服务器上都要安装 "
"Neutron L2 代理程序，对诸如 Open vSwitch 、Linux Bridge 等虚拟网络驱动进行管"
"理。每台节点服务器各运行一个 L2 代理程序，负责管理该节点上的虚拟网络接口。这"
"也是 Neutron L2 代理服务无法实现多节点分布以及高可用的原因。"

msgid "The Pacemaker cluster stack"
msgstr "Packmaker 集群"

msgid ""
"OpenStack infrastructure high availability relies on the <link href=\"http://"
"www.clusterlabs.org\">Pacemaker</link> cluster stack, the state-of-the-art "
"high availability and load balancing stack for the Linux platform. Pacemaker "
"is storage and application-agnostic, and is in no way specific to OpenStack."
msgstr ""
"OpenStack 基础服务的高可用基于 <link href=\"http://www.clusterlabs.org"
"\">Pacemaker</link> 实现（ Pacemaker 是 Linux 系统平台上常见的高可用和负载均"
"衡组件）。 Packermaker 是通用的高可用组件，不限于特定的应用程序或者存储设备，"
"也不是专用于 OpenStack 项目的。"

msgid ""
"Pacemaker relies on the <link href=\"http://www.corosync.org\">Corosync</"
"link> messaging layer for reliable cluster communications. Corosync "
"implements the Totem single-ring ordering and membership protocol. It also "
"provides UDP and InfiniBand based messaging, quorum, and cluster membership "
"to Pacemaker."
msgstr ""
"Pacemaker 使用  <link href=\"http://www.corosync.org\">Corosync</link> 作为消"
"息传输层，以实现集群内部通信的可靠性。Corosync 通过 Totem 协议来维护集群通信"
"的消息序列以及集群成员变动，支持通过 UDP 或 InfiniBand 传输消息，为 "
"Pacemaker 提供集群成员以及合法节点等信息。"

msgid "RabbitMQ"
msgstr "RabbitMQ"

msgid ""
"RabbitMQ is the default AMQP server used by many OpenStack services. Making "
"the RabbitMQ service highly available involves the following steps:"
msgstr ""
"RabbitMQ 是多数 OpenStack 服务的默认 AMQP 服务程序。实现 RabbitMQ 的高可用包"
"括以下步骤："

msgid "Install RabbitMQ"
msgstr "安装 RabbitMQ"

msgid "Configure RabbitMQ for HA queues"
msgstr "配置 RabbitMQ 实现高可用的消息队列"

msgid "Configure OpenStack services to use Rabbit HA queues"
msgstr "配置其它 OpenStack 服务使用高可用的 RabbitMQ 服务"

msgid "Introduction to OpenStack High Availability"
msgstr "OpenStack高可用介绍"

msgid "High Availability systems seek to minimize two things:"
msgstr "实现系统高可用是为了减少以下 2 种异常情况："

msgid "System downtime"
msgstr "系统停机"

msgid ""
"Occurs when a user-facing service is unavailable beyond a specified maximum "
"amount of time."
msgstr "面向客户的服务无法正常工作的时间超出服务承诺的上限。"

msgid "Data loss"
msgstr "数据丢失"

msgid "Accidental deletion or destruction of data."
msgstr "意外发生的数据删除和数据损坏。"

msgid ""
"Most high availability systems guarantee protection against system downtime "
"and data loss only in the event of a single failure. However, they are also "
"expected to protect against cascading failures, where a single failure "
"deteriorates into a series of consequential failures."
msgstr ""
"大多数的高可用系统只能在发生单一故障的情况下为降低停机时间和避免数据丢失提供"
"保障。但是用户也期望高可用系统同样能够处理由单一故障演变为一系列连锁故障的情"
"况。"

msgid ""
"A crucial aspect of high availability is the elimination of single points of "
"failure (SPOFs). A SPOF is an individual piece of equipment or software "
"which will cause system downtime or data loss if it fails. In order to "
"eliminate SPOFs, check that mechanisms exist for redundancy of:"
msgstr ""
"在高可用系统中，最基本的原则是排除单点故障。所谓单点故障，是指系统中的某一单"
"独部件（硬件设备或者软件组件），当它发生故障时会导致系统停机或者数据丢失。可"
"以通过检查下列系统组成部分中是否包含冗余机制来消除单点故障："

msgid "Network components, such as switches and routers"
msgstr "网络设备，如交换机、路由器；"

msgid "Applications and automatic service migration"
msgstr "应用程序以及服务自动迁移工具；"

msgid "Storage components"
msgstr "存储设备；"

msgid "Facility services such as power, air conditioning, and fire protection"
msgstr "辅助设施，如电源、空调、防火等；"

msgid ""
"Most high availability systems will fail in the event of multiple "
"independent (non-consequential) failures. In this case, most systems will "
"protect data over maintaining availability."
msgstr ""
"大多数高可用系统都无法应对发生一连串不相关故障的情况，此时保护数据优先于保证"
"系统的高可用性。"

msgid ""
"High-availability systems typically achieve an uptime percentage of 99.99% "
"or more, which roughly equates to less than an hour of cumulative downtime "
"per year. In order to achieve this, high availability systems should keep "
"recovery times after a failure to about one to two minutes, sometimes "
"significantly less."
msgstr ""
"通常，高可用系统能够保证 99.99% 的在线时间，相当于一年之中发生系统故障的累积"
"时间不超过 1 个小时。要达到这一目标，高可用系统应将故障恢复时间控制在 1 ～ 2 "
"分钟之内甚至更短。"

msgid ""
"OpenStack currently meets such availability requirements for its own "
"infrastructure services, meaning that an uptime of 99.99% is feasible for "
"the OpenStack infrastructure proper. However, OpenStack does not guarantee "
"99.99% availability for individual guest instances."
msgstr ""
"OpenStack 的基础服务，在合理配置的情况下，能够满足上述 99.99% 在线时间的高可"
"用性要求。但是 OpenStack 不能保证单个虚拟机实例的 99.99% 在线时间。"

msgid ""
"Preventing single points of failure can depend on whether or not a service "
"is stateless."
msgstr "避免单点故障的方法根据该服务是否属于无状态类型而有所不同。"

msgid "Stateless vs. Stateful services"
msgstr "无状态和有状态服务"

msgid ""
"A stateless service is one that provides a response after your request, and "
"then requires no further attention. To make a stateless service highly "
"available, you need to provide redundant instances and load balance them. "
"OpenStack services that are stateless include <systemitem class=\"service"
"\">nova-api</systemitem>, <systemitem class=\"service\">nova-conductor</"
"systemitem>, <systemitem class=\"service\">glance-api</systemitem>, "
"<systemitem class=\"service\">keystone-api</systemitem>, <systemitem class="
"\"service\">neutron-api</systemitem> and <systemitem class=\"service\">nova-"
"scheduler</systemitem>."
msgstr ""
"无状态服务是指，当该服务对一个请求作出响应之后，不会再有任何相关操作。实现无"
"状态服务的高可用，只需要同时运行该服务的多个实例，并保证这些实例的负载均衡即"
"可。OpenStack 中无状态的服务包括： <systemitem class=\"service\">nova-api</"
"systemitem>, <systemitem class=\"service\">nova-conductor</systemitem>, "
"<systemitem class=\"service\">glance-api</systemitem>, <systemitem class="
"\"service\">keystone-api</systemitem>, <systemitem class=\"service\">neutron-"
"api</systemitem> and <systemitem class=\"service\">nova-scheduler</"
"systemitem> 。"

msgid ""
"A stateful service is one where subsequent requests to the service depend on "
"the results of the first request. Stateful services are more difficult to "
"manage because a single action typically involves more than one request, so "
"simply providing additional instances and load balancing will not solve the "
"problem. For example, if the Horizon user interface reset itself every time "
"you went to a new page, it wouldn't be very useful. OpenStack services that "
"are stateful include the OpenStack database and message queue."
msgstr ""
"有状态服务，是指客户端发送的后续请求依赖于之前相关请求的处理结果。由于单独一"
"项操作可能涉及若干相关请求，有状态服务相对难于管理，只是通过多个实例和负载均"
"衡无法实现高可用。例如，如果每次访问 Horizon 时都是打开一个全新的页面（之前的"
"操作都消失了），对于用户来说是毫无意义的。OpenStack 中有状态服务包括 "
"OpenStack 数据库和消息队列。"

msgid ""
"Making stateful services highly available can depend on whether you choose "
"an active/passive or active/active configuration."
msgstr "实现有状态服务高可用的方案有“主/从”和“主/主” 2 种模式。"

msgid "Active/Passive"
msgstr "主/从"

msgid ""
"In an active/passive configuration, systems are set up to bring additional "
"resources online to replace those that have failed. For example, OpenStack "
"would write to the main database while maintaining a disaster recovery "
"database that can be brought online in the event that the main database "
"fails."
msgstr ""
"在“主/从”模式中，当系统中的资源失效时，新的资源会被激活，替代失效部份继续提供"
"服务。例如，在 OpenStack 集群中，可以在主数据库之外维护一套灾备数据库，当主数"
"据库发生故障时，激活灾备数据库可以保证集群继续正常运行。"

msgid ""
"Typically, an active/passive installation for a stateless service would "
"maintain a redundant instance that can be brought online when required. "
"Requests may be handled using a virtual IP address to facilitate return to "
"service with minimal reconfiguration required."
msgstr ""
"通常情况下，针对无状态服务实现“主/从”模式的高可用是维护该服务的一个冗余实例，"
"在必要时，这一实例会被激活。客户端的请求统一发送到一个虚拟的 IP 地址（该地址"
"指向实际的后端服务），这样当发生切换时，后端服务和客户端几乎不需要进行任何改"
"动。"

msgid ""
"A typical active/passive installation for a stateful service maintains a "
"replacement resource that can be brought online when required. A separate "
"application (such as Pacemaker or Corosync) monitors these services, "
"bringing the backup online as necessary."
msgstr ""
"有状态服务的“主/从”模式高可用则是维护一套额外的备份资源，当故障发生时，可以直"
"接替代失效部份继续工作。单独的应用程序（如 Pacemaker 、Corosync 等）负责监控"
"各项服务，并在发生故障时激活备份资源。"

msgid "Active/Active"
msgstr "主/主"

msgid ""
"In an active/active configuration, systems also use a backup but will manage "
"both the main and redundant systems concurrently. This way, if there is a "
"failure the user is unlikely to notice. The backup system is already online, "
"and takes on increased load while the main system is fixed and brought back "
"online."
msgstr ""
"在“主/主”模式中，服务的冗余实例和主实例会同时工作。这样主实例发生故障，不会对"
"用户产生影响，因为冗余实例一直处于在线状态，后续客户端的请求直接由冗余实例处"
"理，而主实例的故障恢复可以同步进行。"

msgid ""
"Typically, an active/active installation for a stateless service would "
"maintain a redundant instance, and requests are load balanced using a "
"virtual IP address and a load balancer such as HAProxy."
msgstr ""
"通常，无状态服务“主/主”模式的高可用会维护冗余的服务实例，同时通过虚拟 IP 地址"
"以及负载调度程序（如 HAProxy ）对客户端的请求进行负载均衡。"

msgid ""
"A typical active/active installation for a stateful service would include "
"redundant services with all instances having an identical state. For "
"example, updates to one instance of a database would also update all other "
"instances. This way a request to one instance is the same as a request to "
"any other. A load balancer manages the traffic to these systems, ensuring "
"that operational systems always handle the request."
msgstr ""
"而有状态服务的“主/主”模式高可用则是维护多个完全相同的冗余实例。例如，更新其中"
"一个数据库实例时，其它所有实例都会被更新。这样客户端发送给其中一个实例的请求"
"相当于发给了所有实例。负载调度程序管理客户端和这些实例之间的连接，确保请求发"
"送到正常运行的服务实例。"

msgid ""
"These are some of the more common ways to implement these high availability "
"architectures, but they are by no means the only ways to do it. The "
"important thing is to make sure that your services are redundant, and "
"available; how you achieve that is up to you. This document will cover some "
"of the more common options for highly available systems."
msgstr ""
"上面提到的是较为常见的高可用实现方案，但是并非只有这些方案可以实现系统的高可"
"用。基本原则只是保证服务冗余和可用，具体如何实现则是视需求而定的。本文档会提"
"供如何实现高可用系统的一些通用建议。"

msgid "OpenStack High Availability Guide"
msgstr "OpenStack高可用指南"

msgid "OpenStack Contributors"
msgstr "OpenStack贡献者"

msgid "2012"
msgstr "2012"

msgid "2013"
msgstr "2013"

msgid "2014"
msgstr "2014"

msgid "current"
msgstr "current"

msgid "OpenStack"
msgstr "OpenStack"

msgid "Copyright details are filled in by the template."
msgstr "版权信息来自于模板"

msgid ""
"This guide describes how to install, configure, and manage OpenStack for "
"high availability."
msgstr "本手册将对如何实现 OpenStack 各服务的高可用进行说明。"

msgid "2014-10-17"
msgstr "2014-10-17"

msgid ""
"This guide has gone through editorial changes to follow the OpenStack "
"documentation conventions. Various smaller issues have been fixed."
msgstr "本手册根据 OpenStack 文档规范进行了修订，改正了不少细微的错误。"

msgid "2014-05-16"
msgstr "2014-05-16"

msgid "2014-04-17"
msgstr "2014-04-17"

msgid ""
"Minor cleanup of typos, otherwise no major revisions for Icehouse release."
msgstr "清理一些拼写错误，相对于 Icehouse 版本没有大的改动。"

msgid "2012-01-16"
msgstr "2012-01-16"

msgid "Organizes guide based on cloud controller and compute nodes."
msgstr "本指南将介绍如何安装控制节点和计算节点"

msgid "2012-05-24"
msgstr "2012-05-24"

msgid "Begin trunk designation."
msgstr "开始主干指定。"

msgid "Network controller cluster stack"
msgstr "网络控制节点 HA 集群配置"

msgid ""
"The network controller sits on the management and data network, and needs to "
"be connected to the Internet if an instance will need access to the Internet."
msgstr ""
"网络控制节点运行在管理网络和数据网络中，如果虚拟机实例要连接到互联网，网络控"
"制节点也需要具备互联网连接。"

msgid ""
"Pacemaker requires that both nodes have different hostnames. Because of "
"that, RA scripts could require some adjustments since the Networking "
"scheduler will be aware of one node, for example a virtual router attached "
"to a single L3 node. For example, both nodes could set different hostnames "
"in the configuration files, and when the l3-agent started by Pacemaker, the "
"node's hostname will be changed to network-controller automatically. "
"Whichever node starts the l3-agent will have the same hostname."
msgstr ""
"Pacemaker 要求所有的节点服务器使用不用的主机名，而 OpenStack 网络服务调度程序"
"只会关注其中一台（例如，一个虚拟路由器只能在其中一台运行 L3 代理的服务上启"
"动），因此需要对 RA （Pacemaker 资源代理程序） 脚本进行相应修改。比如，所有的"
"节点先各自在配置文件中配置不同的主机名，当 Pacemaker 启动 l3-agent 时，自动将"
"该节点的主机名改为 network-controller，这样所有启动 l3-agent 的节点会使用相同"
"的主机名。"

msgid "HAProxy nodes"
msgstr "HAProxy 节点服务器"

msgid ""
"For installing HAProxy on your nodes, you should consider its <link href="
"\"http://haproxy.1wt.eu/#docs\">official documentation</link>. Also, you "
"have to consider that this service should not be a single point of failure, "
"so you need at least two nodes running HAProxy."
msgstr ""
"HAProxy 的安装，请参阅项目<link href=\"http://haproxy.1wt.eu/#docs\">官方文档"
"</link>。别外，为了避免 HAProxy 本身成为整个系统的单点故障，最少应配置 2 台 "
"HAProxy 节点服务器。"

msgid "Here is an example of the HAProxy configuration file:"
msgstr "下面是 HAProxy 配置文件的示例："

msgid "After each change of this file, you should restart HAProxy."
msgstr "每次修改配置文件之后，必须重启 HAProxy 服务。"

msgid "OpenStack controller nodes"
msgstr "OpenStack 控制节点服务器"

msgid "OpenStack controller nodes contain:"
msgstr "OpenStack 控制节点服务器上运行以下服务："

msgid "All OpenStack API services"
msgstr "所有 OpenStack API 服务"

msgid "All OpenStack schedulers"
msgstr "所有 OpenStack 调度相关的服务"

msgid "<application>Memcached</application> service"
msgstr "<application>Memcached</application> 服务"

msgid "HA using active/passive"
msgstr "主/从模式高可用集群"

msgid "API node cluster stack"
msgstr "API 服务节点 HA 集群配置"

msgid ""
"The API node exposes OpenStack API endpoints onto external network "
"(Internet). It must talk to the cloud controller on the management network."
msgstr ""
"API 服务节点对外（整个互联网）提供 OpenStack API 接口。它们通过管理网络和 "
"OpenStack 控制节点进行交互。"

msgid "Database"
msgstr "数据库"

msgid ""
"The choice of database isn't a foregone conclusion; you're not required to "
"use <application>MySQL</application>. It is, however, a fairly common choice "
"in OpenStack installations, so we'll cover it here."
msgstr ""
"<application>MySQL</application> 并不是唯一的选择，以它作为示例是因为目前已有"
"的 OpenStack 布署案例中，使用 <application>MySQL</application> 作为数据库比较"
"常见。"

msgid ""
"MySQL with Galera is by no means the only way to achieve database HA. "
"MariaDB Galera Cluster (<link href=\"https://mariadb.org/\">https://mariadb."
"org/</link>) and Percona XtraDB Cluster (<link href=\"http://www.percona.com/"
"\">http://www.percona.com/</link>) also work with Galera. You also have the "
"option to use PostgreSQL, which has its own replication, or another database "
"HA option."
msgstr ""
"同样，MySQL + Galera 也并非唯一一种实现数据库高可用的方案。MariaDB Galera 集"
"群(<link href=\"https://mariadb.org/\">https://mariadb.org/</link>) 和 "
"Percona XtraDB 集群(<link href=\"http://www.percona.com/\">http://www."
"percona.com/</link>) 都可以和 Galera 一起配置使用。另外 OpenStack 也支持 "
"PostgreSQL，PostgreSQL 有自己相应的数据镜像和高可用方案。"

msgid "HA using active/active"
msgstr "主/主模式高可用集群"

msgid "MySQL with Galera"
msgstr "MySQL 和 Galera"

msgid ""
"Rather than starting with a vanilla version of MySQL, and then adding "
"Galera, you will want to install a version of MySQL patched for wsrep (Write "
"Set REPlication) from <link href=\"https://launchpad.net/codership-mysql"
"\">https://launchpad.net/codership-mysql</link>. The wsrep API is suitable "
"for configuring MySQL High Availability in OpenStack because it supports "
"synchronous replication."
msgstr ""
"相比安装 MySQL 普通版本，然后安装 Galera 群集复制，更合适你的是从 <link href="
"\"https://launchpad.net/codership-mysql\">https://launchpad.net/codership-"
"mysql</link>  安装已经打了wsrep （(Write Set REPlication) 补丁的 MySQL , "
"wsrep API 在 OpenStack  中配置 MySQL 高可用是合适的，因为它支持同步复制。"

msgid ""
"Note that the installation requirements call for careful attention. Read the "
"guide <link href=\"https://launchpadlibrarian.net/66669857/README-wsrep"
"\">https://launchpadlibrarian.net/66669857/README-wsrep</link> to ensure you "
"follow all the required steps."
msgstr ""
"请仔细阅读 Wresp API 的安装说明 —— <link href=\"https://launchpadlibrarian."
"net/66669857/README-wsrep\">https://launchpadlibrarian.net/66669857/README-"
"wsrep</link>，并按要求完成安装。"

msgid ""
"And for any additional information about Galera, you can access this guide: "
"<link href=\"http://galeracluster.com/documentation-webpages/gettingstarted."
"html\">http://galeracluster.com/documentation-webpages/gettingstarted.html</"
"link>"
msgstr ""
"更多关于 Galera 的信息，请参阅：<link href=\"http://galeracluster.com/"
"documentation-webpages/gettingstarted.html\">http://galeracluster.com/"
"documentation-webpages/gettingstarted.html</link>"

msgid "Installing Galera through a MySQL version patched for wsrep:"
msgstr "为已经加上 wresp 补丁的 MySQL 数据库安装 Galera ："

msgid ""
"You can change the mirror to one near you on: <link href=\"https://downloads."
"mariadb.org/mariadb/repositories/\">downloads.mariadb.org</link>"
msgstr ""
"您可以改变镜像到离您最近的一个：<link href=\"https://downloads.mariadb.org/"
"mariadb/repositories/\">downloads.mariadb.org</link>"

msgid "Update your system and install the required packages: <placeholder-1/>"
msgstr "升级您的系统并安装必要的软件包：<placeholder-1/>"

msgid ""
"If you have mariaDB already installed you need to re-apply all the "
"permissions from the installation guide. It will purge all privileges!"
msgstr ""
"如果您已经安装了mariaDB，您需要重新申请安装指南中的所有权限。此将清理所有的权"
"限！"

msgid "Adjust the configuration:"
msgstr "调整配置文件："

msgid ""
"In the <filename>/etc/mysql/my.conf</filename> file, make the following "
"changes:"
msgstr "在<filename>/etc/mysql/my.conf</filename>文件中，进行如下修改："

msgid "Create the <filename>/etc/mysql/conf.d/wsrep.cnf</filename> file."
msgstr "创建<filename>/etc/mysql/conf.d/wsrep.cnf</filename>文件"

msgid "Paste the following lines in this file:"
msgstr "将如下内容粘贴到文件中："

msgid "PRIMARY_NODE_IP"
msgstr "PRIMARY_NODE_IP"

msgid "SECONDARY_NODE_IP"
msgstr "SECONDARY_NODE_IP"

msgid "NODE_NAME"
msgstr "NODE_NAME"

msgid ""
"Replace <replaceable>PRIMARY_NODE_IP</replaceable> and "
"<replaceable>SECONDARY_NODE_IP</replaceable> with the IP addresses of your "
"primary and secondary servers."
msgstr ""
"使用您主服务器和次服务器的IP地址替换<replaceable>PRIMARY_NODE_IP</"
"replaceable> 和 <replaceable>SECONDARY_NODE_IP</replaceable>"

msgid ""
"Replace <replaceable>PRIMARY_NODE_IP</replaceable> with the hostname of the "
"server. This is set for logging."
msgstr ""
"使用服务器的主机名替换<replaceable>PRIMARY_NODE_IP</replaceable>。这项配置用"
"于日志。"

msgid ""
"Copy this file to all other databases servers and change the value of "
"<literal>wsrep_cluster_address</literal> and <literal>wsrep_node_name</"
"literal> accordingly."
msgstr ""
"将此文件拷贝到所有的其他数据库服务器上并相应的修改"
"<literal>wsrep_cluster_address</literal> 和<literal>wsrep_node_name</literal>"
"的值。"

msgid "Start mysql as root and execute the following queries:"
msgstr "使用root启动mysql并执行以下查询："

msgid "Remove user accounts with empty user names because they cause problems:"
msgstr "删除所有用户名为空的 MySQL 帐户（这些帐户会产生安全隐患）："

msgid ""
"Check that the nodes can access each other through the firewall. Depending "
"on your environment, this might mean adjusting iptables, as in:"
msgstr ""
"检查各节点之间的网络通信有没有被防火墙拦截。根据布署环境的不同，检查方法也各"
"不相同。如，某些环境中，这一步只需要对 iptables 进行配置："

msgid ""
"This might also mean configuring any NAT firewall between nodes to allow "
"direct connections. You might need to disable SELinux, or configure it to "
"allow <systemitem class=\"service\">mysqld</systemitem> to listen to sockets "
"at unprivileged ports."
msgstr ""
"在某些环境中，可能还需要对 NAT 防火墙进行配置，以保证节点服务器之间可以直接通"
"信。另外，可能需要禁用 SELinux，或者允许 <systemitem class=\"service"
"\">mysqld</systemitem> 监听非特权端口。"

msgid ""
"After the copy make sure that all files are the same, you can do this by "
"using the following command:"
msgstr "复制之后需确保所有的文件都是相同的，你能通过使用如下命令确认："

msgid "From the <filename>debian.cnf</filename> get the database password:"
msgstr "从<filename>debian.cnf</filename>中获取数据库密码："

msgid "The result will look like this:"
msgstr "结果将如下所示："

msgid ""
"Stop all the mysql servers and start the first server with the following "
"command:"
msgstr "停止所有的mysql服务器然后使用如下命令启动第一个服务器："

msgid "All other nodes can now be started using:"
msgstr "其他所有节点现在能被启动："

msgid ""
"Verify the wsrep replication by logging in as root under mysql and running "
"the following command:"
msgstr "通过以root身份登录mysql并运行以下命令来确认wsrep复制："

msgid "PASSWORD"
msgstr "密码"

msgid "Run neutron L3 agent"
msgstr "运行 neutron L3 代理服务"

msgid "Option"
msgstr "选项"

msgid "Description"
msgstr "描述"

msgid "l3_ha"
msgstr "l3_ha"

msgid "max_l3_agents_per_router"
msgstr "max_l3_agents_per_router"

msgid "2"
msgstr "2"

msgid "min_l3_agents_per_router"
msgstr "min_l3_agents_per_router"

msgid "Run neutron metadata agent"
msgstr "运行 neutron 元数据代理服务"

msgid ""
"There is no native feature to make this service highly available. At this "
"time, the Active / Passive solution exists to run the neutron metadata agent "
"in failover mode with <application>Pacemaker</application>. See the <link "
"linkend=\"ha-using-active-passive\">active/passive section</link> of this "
"guide."
msgstr ""
"Neutron 元数据代理服务自身是不支持高可用的。目前针对 Neutron L3 代理服务只有"
"主/从模式的高可用方案通过 <application>Pacemaker</application> 可以实现失效切"
"换。参阅本手册的 <link linkend=\"ha-using-active-passive\">主/从模式高可用架"
"构部分</link>。"

msgid "Run neutron DHCP agent"
msgstr "运行 neutron DHCP 代理服务"

msgid ""
"The OpenStack Networking service has a scheduler that lets you run multiple "
"agents across nodes. Also, the DHCP agent can be natively highly available. "
"You can configure the number of DHCP agents per network using the parameter "
"<placeholder-1/> in <filename>neutron.conf</filename>. By default this is "
"equal to 1. To achieve high availability assign more than one DHCP agent per "
"network."
msgstr ""
"OpenStack 网络服务拥有一个调度程序，所有可以在多个节点同时运行各种代理程序。"
"同样，DHCP 代理服务自身就是支持高可用的。每个网络使用多少 DHCP 代理程序是可以"
"通过配置文件 <filename>neutron.conf</filename> 中的 <placeholder-1/> 进行配置"
"的。默认值是 1 ，要实现 DHCP 代理服务的高可用，应为每个网络设置多个 DHCP 代理"
"程序。"

msgid "Run neutron LBaaS agent"
msgstr "运行 neutron LBaas 代理服务"

msgid "Highly available Block Storage API"
msgstr "高可用 OpenStack 块设备存储服务"

msgid ""
"Making the Block Storage (cinder) API service highly available in active / "
"passive mode involves:"
msgstr "使得块存储(cinder)API服务在主/被模式中高可用包括:"

msgid "Configuring Block Storage to listen on the VIP address"
msgstr "配置块存储监听于VIP地址"

msgid "Managing Block Storage API daemon with the Pacemaker cluster manager"
msgstr "使用 Pacemaker 管理 OpenStack 块设备存储服务"

msgid "Configuring OpenStack services to use this IP address"
msgstr "使用该 IP 地址配置 OpenStack 服务"

msgid "Add Block Storage API resource to Pacemaker"
msgstr "在 Pacemaker 中添加 OpenStack 块设备存储服务资源"

msgid "First of all, you need to download the resource agent to your system:"
msgstr "首先，下载 Pacemaker 资源代理："

msgid ""
"You can now add the Pacemaker configuration for Block Storage API resource. "
"Connect to the Pacemaker cluster with <literal>crm configure</literal>, and "
"add the following cluster resources:"
msgstr ""
"现在可以在 Pacemaker 中填加 OpenStack 块设备存储服务相关资源。执行 "
"<literal>crm configure</literal> 命令进入 Pacemaker 配置菜单，然后加入下列集"
"群资源："

msgid "This configuration creates"
msgstr "这个配置创建"

msgid ""
"<literal>p_cinder-api</literal>, a resource for manage Block Storage API "
"service"
msgstr ""
"<literal>p_cinder-api</literal> 资源，对 OpenStack 身份认证服务进行管理。"

msgid ""
"<literal>crm configure</literal> supports batch input, so you may copy and "
"paste the above into your live pacemaker configuration, and then make "
"changes as required. For example, you may enter <literal>edit p_ip_cinder-"
"api</literal> from the <literal>crm configure</literal> menu and edit the "
"resource to match your preferred virtual IP address."
msgstr ""
"<literal>crm configure</literal> 支持批量输入，因此可以拷贝粘贴上面到现有的 "
"Pacemaker 配置中，然后根据需要再作修改。例如，可以从 <literal>crm configure</"
"literal> 菜单中进入 <literal>edit p_ip_cinder-api</literal>，编辑资源以匹配可"
"供使用的虚拟IP地址。"

msgid ""
"Once completed, commit your configuration changes by entering "
"<literal>commit</literal> from the <literal>crm configure</literal> menu. "
"Pacemaker will then start the Block Storage API service, and its dependent "
"resources, on one of your nodes."
msgstr ""
"配置完成后，在 <literal>crm configure</literal>  菜单下输入 <literal>commit</"
"literal> 提交所有配置变更。随后 Pacemaker 会其中一台节点服务器上启动"
"OpenStack 块设备存储存服务（包括所有相关资源）。"

msgid "Configure Block Storage API service"
msgstr "配置 OpenStack 块设备存储服务"

msgid "Edit <filename>/etc/cinder/cinder.conf</filename>:"
msgstr "编辑 <filename>/etc/cinder/cinder.conf</filename>："

msgid "Configure OpenStack services to use highly available Block Storage API"
msgstr "配置 OpenStack 各服务使用高可用的 OpenStack 块设备存储服务"

msgid ""
"Your OpenStack services must now point their Block Storage API configuration "
"to the highly available, virtual cluster IP address — rather than a Block "
"Storage API server’s physical IP address as you normally would."
msgstr ""
"其它 OpenStack 服务也相应地使用高可用、使用虚拟 IP 地址的 OpenStack 块设备存"
"储服务，而不在使用其所在服务器的物理 IP 地址。"

msgid "You must create the Block Storage API endpoint with this IP."
msgstr "在 OpenStack 身份认证服务中需要为该 IP 地址创建对应的服务端点。"

msgid ""
"If you are using both private and public IP, you should create two Virtual "
"IPs and define your endpoint like this:"
msgstr ""
"如果要同时使用私有和公开的 IP 地址，需要创建两个虚拟 IP 地址资源，并建立类似"
"如下的服务端点："

msgid "Configure Pacemaker group"
msgstr "配置 Pacemaker 资源组"

msgid ""
"Finally, we need to create a service <literal>group</literal> to ensure that "
"the virtual IP is linked to the API services resources:"
msgstr ""
"最后，创建一个资源组 <literal>group</literal>，将所有 API 服务资源和该虚拟 "
"IP 地址联系起来。"

msgid "Highly available OpenStack Image API"
msgstr "高可用 OpenStack 镜像 API 服务"

msgid "Configure OpenStack services to use this IP address."
msgstr "配置 OpenStack 服务使用该虚拟 IP 地址。"

msgid "Add OpenStack Image API resource to Pacemaker"
msgstr "在 Pacemaker 中添加 OpenStack 镜像服务资源"

msgid ""
"<literal>p_glance-api</literal>, a resource for managing OpenStack Image API "
"service"
msgstr "<literal>p_glance-api</literal> 资源，对 OpenStack 镜像服务进行管理。"

msgid ""
"<literal>crm configure</literal> supports batch input, so you may copy and "
"paste the above into your live Pacemaker configuration, and then make "
"changes as required. For example, you may enter <literal>edit p_ip_glance-"
"api</literal> from the <literal>crm configure</literal> menu and edit the "
"resource to match your preferred virtual IP address."
msgstr ""
"<literal>crm configure</literal> 支持批量输入，因此可以拷贝粘贴上面到现有的 "
"Pacemaker 配置中，然后根据需要再作修改。例如，可以从 <literal>crm configure</"
"literal> 菜单中进入 <literal>edit p_ip_glance-api</literal>，编辑资源以匹配可"
"供使用的虚拟IP地址。"

msgid ""
"Once completed, commit your configuration changes by entering "
"<literal>commit</literal> from the <literal>crm configure</literal> menu. "
"Pacemaker will then start the OpenStack Image API service, and its dependent "
"resources, on one of your nodes."
msgstr ""
"配置完成后，在 <literal>crm configure</literal>  菜单下输入 <literal>commit</"
"literal> 提交所有配置变更。随后 Pacemaker 会其中一台节点服务器上启动"
"OpenStack 镜像服务（包括所有相关资源）。"

msgid "Edit <filename>/etc/glance/glance-api.conf</filename>:"
msgstr "编辑 <filename>/etc/glance/glance-api.conf</filename>："

msgid "Configure OpenStack services to use high available OpenStack Image API"
msgstr "配置 OpenStack 各服务使用高可用的 OpenStack镜像服务"

msgid ""
"Your OpenStack services must now point their OpenStack Image API "
"configuration to the highly available, virtual cluster IP address — rather "
"than an OpenStack Image API server’s physical IP address as you normally "
"would."
msgstr ""
"其它 OpenStack 服务也相应地使用高可用、使用虚拟 IP 地址的 OpenStack 镜像服"
"务，而不在使用其所在服务器的物理 IP 地址。"

msgid ""
"For OpenStack Compute, for example, if your OpenStack Image API service IP "
"address is <literal>192.168.42.103</literal> as in the configuration "
"explained here, you would use the following configuration in your "
"<filename>nova.conf</filename> file:"
msgstr ""
"以 OpenStack 计算服务为例，如果 OpenStack 镜像服务的虚拟 IP 地址是 "
"<literal>192.168.42.103</literal>，那么在 OpenStack 计算服务的配置文件"
"（ <filename>nova.conf</filename> ）中应该使用如下配置："

msgid ""
"In versions prior to Juno, this option was called "
"<literal>glance_api_servers</literal> in the <literal>[DEFAULT]</literal> "
"section."
msgstr ""
"对于 Juno 之前的版本，该配置项对应的是 <literal>[DEFAULT]</literal> 段之下的 "
"<literal>glance_api_servers</literal>。"

msgid "You must also create the OpenStack Image API endpoint with this IP."
msgstr "在 OpenStack 身份认证服务中需要为该 IP 地址创建对应的服务端点。"

msgid ""
"If you are using both private and public IP addresses, you should create two "
"Virtual IP addresses and define your endpoint like this:"
msgstr ""
"如果要同时使用私有和公开的 IP 地址，需要创建两个虚拟 IP 地址资源，并建立类似"
"如下的服务端点："

msgid "Highly available OpenStack Networking server"
msgstr "高可用 OpenStack 网络服务"

msgid ""
"OpenStack Networking is the network connectivity service in OpenStack. "
"Making the OpenStack Networking Server service highly available in active / "
"passive mode involves the following tasks:"
msgstr ""
"OpenStack 网络服务为 OpenStack 集群提供网络基础服务。实现 OpenStack 网络服务"
"主/从模式的高可用包括以下步骤："

msgid "Configure OpenStack Networking to listen on the virtual IP address,"
msgstr "配置 OpenStack 网络服务监听虚拟 IP 地址，"

msgid ""
"Manage the OpenStack Networking API Server daemon with the Pacemaker cluster "
"manager,"
msgstr "使用 Pacemaker 管理 OpenStack 网络服务，"

msgid "Configure OpenStack services to use the virtual IP address."
msgstr "配置 OpenStack 服务使用该虚拟  IP 地址。"

msgid "Add OpenStack Networking Server resource to Pacemaker"
msgstr "在 Pacemaker 中添加 OpenStack 网络服务资源"

msgid ""
"You can now add the Pacemaker configuration for OpenStack Networking Server "
"resource. Connect to the Pacemaker cluster with <literal>crm configure</"
"literal>, and add the following cluster resources:"
msgstr ""
"现在可以在 Pacemaker 中填加 OpenStack 网络服务相关资源。执行 <literal>crm "
"configure</literal> 命令进入 Pacemaker 配置菜单，然后加入下列集群资源："

msgid ""
"This configuration creates <literal>p_neutron-server</literal>, a resource "
"for manage OpenStack Networking Server service"
msgstr ""
"该配置增加  <literal>p_neutron-server</literal> 资源，对 OpenStack 网络服务进"
"行管理。"

msgid ""
"<literal>crm configure</literal> supports batch input, so you may copy and "
"paste the above into your live pacemaker configuration, and then make "
"changes as required. For example, you may enter <literal>edit p_neutron-"
"server</literal> from the <literal>crm configure</literal> menu and edit the "
"resource to match your preferred virtual IP address."
msgstr ""
"<literal>crm configure</literal> 支持批量输入，因此可以拷贝粘贴上面到现有的 "
"Pacemaker 配置中，然后根据需要再作修改。例如，可以从 <literal>crm configure</"
"literal> 菜单中进入 <literal>edit p_ip_keystone</literal>，编辑资源以匹配可供"
"使用的虚拟IP地址"

msgid ""
"Once completed, commit your configuration changes by entering "
"<literal>commit</literal> from the <literal>crm configure</literal> menu. "
"Pacemaker will then start the OpenStack Networking API service, and its "
"dependent resources, on one of your nodes."
msgstr ""
"配置完成后，在 <literal>crm configure</literal>  菜单下输入 <literal>commit</"
"literal> 提交所有配置变更。随后 Pacemaker 会其中一台节点服务器上启动"
"OpenStack 网络服务（包括所有相关资源）。"

msgid "Configure OpenStack Networking server"
msgstr "配置 OpenStack 网络服务"

msgid "Edit <filename>/etc/neutron/neutron.conf</filename>:"
msgstr "编辑 <filename>/etc/neutron/neutron.conf</filename>："

msgid ""
"Configure OpenStack services to use highly available OpenStack Networking "
"server"
msgstr "配置 OpenStack 各服务使用高可用的 OpenStack 网络服务"

msgid ""
"Your OpenStack services must now point their OpenStack Networking Server "
"configuration to the highly available, virtual cluster IP address — rather "
"than an OpenStack Networking server’s physical IP address as you normally "
"would."
msgstr ""
"其它 OpenStack 服务也相应地使用高可用、使用虚拟 IP 地址的 OpenStack 网络服"
"务，而不在使用其所在服务器的物理 IP 地址。"

msgid ""
"For example, you should configure OpenStack Compute for using highly "
"available OpenStack Networking server in editing <literal>nova.conf</"
"literal> file:"
msgstr ""
"以 OpenStack 计算服务为例，在 OpenStack 计算服务的配置文件（ <literal>nova."
"conf</literal> ）中应该使用如下配置："

msgid ""
"You need to create the OpenStack Networking server endpoint with this IP."
msgstr "在 OpenStack 身份认证服务中需要为该 IP 地址创建对应的服务端点。"

msgid "Highly available Telemetry central agent"
msgstr "高可用 Telemetry 监控代理"

msgid ""
"Telemetry (ceilometer) is the metering and monitoring service in OpenStack. "
"The Central agent polls for resource utilization statistics for resources "
"not tied to instances or compute nodes."
msgstr ""
"Telemtry （ ceilometer ）是 OpenStack 系统中的计量和监控服务。监控中心收集包"
"括虚拟机实例和计算节点在内各种资源的使用情况。"

msgid ""
"Making the Telemetry central agent service highly available in active / "
"passive mode involves managing its daemon with the Pacemaker cluster manager."
msgstr ""
"Telemetry 监控中心的主/从模式高可用是通过 Pacemaker 管理其后台守护进程实现。"

msgid ""
"You will find at <link href=\"http://docs.openstack.org/developer/ceilometer/"
"install/manual.html#installing-the-central-agent\">this page</link> the "
"process to install the Telemetry central agent."
msgstr ""
"参见 Telemtry 服务监控中心的安装<link href=\"http://docs.openstack.org/"
"developer/ceilometer/install/manual.html#installing-the-central-agent\"> 文"
"档 </link>。"

msgid "Add the Telemetry central agent resource to Pacemaker"
msgstr "在 Pacemaker 中添加 Telemetry 监控中心资源"

msgid ""
"You may then proceed with adding the Pacemaker configuration for the "
"Telemetry central agent resource. Connect to the Pacemaker cluster with "
"<literal>crm configure</literal>, and add the following cluster resources:"
msgstr ""
"现在可以在 Pacemaker 中填加 Telemetry 监中心相关资源。执行 <literal>crm "
"configure</literal> 命令进入 Pacemaker 配置菜单，然后加入下列集群资源："

msgid ""
"<literal>p_ceilometer-agent-central</literal>, a resource for managing the "
"Ceilometer Central Agent service"
msgstr ""
"<literal>p_ceilometer-agent-central</literal>, 用来管理 Ceilometer 监控代理服"
"务的资源 "

msgid ""
"<literal>crm configure</literal> supports batch input, so you may copy and "
"paste the above into your live pacemaker configuration, and then make "
"changes as required."
msgstr ""
"<literal>crm configure</literal> 支持批量输入，因此可以拷贝粘贴上面到现有的 "
"Pacemaker 配置中，然后根据需要再作修改。"

msgid ""
"Once completed, commit your configuration changes by entering "
"<literal>commit</literal> from the <literal>crm configure</literal> menu. "
"Pacemaker will then start the Ceilometer Central Agent service, and its "
"dependent resources, on one of your nodes."
msgstr ""
"配置完成后，在 <literal>crm configure</literal>  菜单下输入 <literal>commit</"
"literal> 提交所有配置变更。随后 Pacemaker 会其中一台节点服务器上启动 "
"Telemetry 监控中心（包括所有相关资源）。"

msgid "Configure Telemetry central agent service"
msgstr "配置 Telemetry 监中心"

msgid "Edit <filename>/etc/ceilometer/ceilometer.conf</filename>:"
msgstr "编辑 <filename>/etc/ceilometer/ceilometer.conf</filename>："

msgid "Highly available OpenStack Identity"
msgstr "高可用 OpenStack 身份认证服务"

msgid ""
"OpenStack Identity is the Identity Service in OpenStack and used by many "
"services. Making the OpenStack Identity service highly available in active / "
"passive mode involves"
msgstr ""
"OpenStack 身份认证服务被很多其他服务使用。实现 OpenStack 身份认证服务主/从模"
"式的高可用包括以下步骤："

msgid "Configure OpenStack Identity to listen on the VIP address,"
msgstr "配置 OpenStack 身份认证服务监听虚拟 IP 地址，"

msgid "Managing OpenStack Identity daemon with the Pacemaker cluster manager,"
msgstr "使用 Pacemaker 管理 OpenStack 身份认证服务，"

msgid "Add OpenStack Identity resource to Pacemaker"
msgstr "在 Pacemaker 中添加 OpenStack 认证服务资源"

msgid ""
"You can now add the Pacemaker configuration for OpenStack Identity resource. "
"Connect to the Pacemaker cluster with <literal>crm configure</literal>, and "
"add the following cluster resources:"
msgstr ""
"现在可以在 Pacemaker 中填加 OpenStack 身份认证服务相关资源。执行 "
"<literal>crm configure</literal> 命令进入 Pacemaker 配置菜单，然后加入下列集"
"群资源："

msgid ""
"This configuration creates <literal>p_keystone</literal>, a resource for "
"managing the OpenStack Identity service."
msgstr ""
"该配置增加  <literal>p_keystone</literal> 资源，对 OpenStack 身份认证服务进行"
"管理。"

msgid ""
"<literal>crm configure</literal> supports batch input, so you may copy and "
"paste the above into your live pacemaker configuration, and then make "
"changes as required. For example, you may enter <literal>edit p_ip_keystone</"
"literal> from the <literal>crm configure</literal> menu and edit the "
"resource to match your preferred virtual IP address."
msgstr ""
"<literal>crm configure</literal> 支持批量输入，因此可以拷贝粘贴上面到现有的 "
"Pacemaker 配置中，然后根据需要再作修改。例如，可以从 <literal>crm configure</"
"literal> 菜单中进入 <literal>edit p_ip_keystone</literal>，编辑资源以匹配可供"
"使用的虚拟IP地址。"

msgid ""
"Once completed, commit your configuration changes by entering "
"<literal>commit</literal> from the <literal>crm configure</literal> menu. "
"Pacemaker will then start the OpenStack Identity service, and its dependent "
"resources, on one of your nodes."
msgstr ""
"配置完成后，在 <literal>crm configure</literal>  菜单下输入 <literal>commit</"
"literal> 提交所有配置变更。随后 Pacemaker 会其中一台节点服务器上启动 "
"OpenStack 身份认证服务（包括所有相关资源）。"

msgid "Configure OpenStack Identity service"
msgstr "配置 OpenStack 身份认证服务"

msgid ""
"You need to edit your OpenStack Identity configuration file "
"(<filename>keystone.conf</filename>) and change the bind parameters:"
msgstr ""
"编辑 OpenStack 身份认证服务的配置文件（ <filename>keystone.conf</"
"filename> ），调整以下配置项："

msgid "On Havana:"
msgstr "对于 Havana 版本："

msgid ""
"To be sure all data will be highly available, you should be sure that you "
"store everything in the MySQL database (which is also highly available):"
msgstr ""
"为了保证所有的数据都是高可用的，应使用 MySQL 数据库服务（同样也要保证 MySQL "
"服务是高可用的）："

msgid ""
"Configure OpenStack services to use the highly available OpenStack Identity"
msgstr "配置 OpenStack 各服务使用高可用的 OpenStack 身份认证服务"

msgid ""
"Your OpenStack services must now point their OpenStack Identity "
"configuration to the highly available, virtual cluster IP address — rather "
"than a OpenStack Identity server’s physical IP address as you normally would."
msgstr ""
"其它 OpenStack 服务也相应地使用高可用、使用虚拟 IP 地址的 OpenStack 身份认证"
"服务，而不在使用其所在服务器的物理 IP 地址。"

msgid ""
"For example with OpenStack Compute, if your OpenStack Identity service IP "
"address is <literal>192.168.42.103</literal> as in the configuration "
"explained here, you would use the following line in your API configuration "
"file (<literal>api-paste.ini</literal>):"
msgstr ""
"以 OpenStack 计算服务为例，如果 OpenStack 身份认证服务的虚拟 IP 地址是 "
"<literal>192.168.42.103</literal>，那么在 OpenStack 计算服务的配置文件"
"（ <literal>api-paste.ini</literal> ）中应该使用如下配置："

msgid "You also need to create the OpenStack Identity Endpoint with this IP."
msgstr "在 OpenStack 身份认证服务中需要为该 IP 地址创建对应的服务端点。"

msgid ""
"If you are using the horizon dashboard, you should edit the "
"<literal>local_settings.py</literal> file:"
msgstr ""
"如果配置了 Horizon 面板，也需要修改 Horizon 的配置文件 "
"<literal>local_settings.py</literal> ： "

msgid "Configure the VIP"
msgstr "配置VIP"

msgid ""
"First, you must select and assign a virtual IP address (VIP) that can freely "
"float between cluster nodes."
msgstr "首先选择并绑定一个可以在各集群节点之间迁移的虚拟 IP 地址 （即 VIP ）。"

msgid ""
"This configuration creates <literal>p_ip_api</literal>, a virtual IP address "
"for use by the API node (<literal>192.168.42.103</literal>):"
msgstr ""
"该配置新建了一个 <literal>p_ip_mysql</literal> 资源，是 API 节点将会使用的虚"
"拟 IP 地址（<literal>192.168.42.103</literal>）："

msgid "Highly available RabbitMQ"
msgstr "高可用的 RabbitMQ"

msgid ""
"RabbitMQ is the default AMQP server used by many OpenStack services. Making "
"the RabbitMQ service highly available involves:"
msgstr ""
"RabbitMQ 是多数 OpenStack 服务的默认 AMQP 服务程序。实现 RabbitMQ 的高可用包"
"括以下步骤："

msgid "configuring a DRBD device for use by RabbitMQ,"
msgstr "为 RabbitMQ 配置一个 DRBD 设备"

msgid ""
"configuring RabbitMQ to use a data directory residing on that DRBD device,"
msgstr "配置 RabbitMQ 使用建立在 DRBD 设备之上的数据目录，"

msgid ""
"selecting and assigning a virtual IP address (VIP) that can freely float "
"between cluster nodes,"
msgstr "选择并绑定一个可以在各集群节点之间迁移的虚拟 IP 地址 （即 VIP ），"

msgid "configuring RabbitMQ to listen on that IP address,"
msgstr "配置 RabbitMQ 监听该 IP 地址，"

msgid ""
"managing all resources, including the RabbitMQ daemon itself, with the "
"Pacemaker cluster manager."
msgstr "使用 Pacemaker 管理上述所有资源，包括 RabbitMQ 守护进程本身。"

msgid ""
"<link href=\"http://www.rabbitmq.com/ha.html\">Active-active mirrored "
"queues</link> is another method for configuring RabbitMQ versions 3.3.0 and "
"later for high availability. You can also manage a RabbitMQ cluster with "
"active-active mirrored queues using the Pacemaker cluster manager."
msgstr ""
"<link href=\"http://www.rabbitmq.com/ha.html\">主/主镜像队列</link> 是实现 "
"RabbitMQ 高可用的另一种可选方案（适用于 RabbitMQ 3.3.0 及之后的版本），同样，"
"也可以通过 Pacemaker 来管理使用“主/主镜像队列”架构的 RabbitMQ 集群。"

msgid "Configure DRBD"
msgstr "配置 DRBD"

msgid ""
"The Pacemaker based RabbitMQ server requires a DRBD resource from which it "
"mounts the <filename>/var/lib/rabbitmq</filename> directory. In this "
"example, the DRBD resource is simply named <literal>rabbitmq</literal>:"
msgstr ""
"基于 Pacemaker 的 RabbitMQ 服务需要一个 DRBD 设备，并将之挂载到 <filename>/"
"var/lib/rabbitmq</filename> 目录。在示例中，DRBD 资源被简单命名为 "
"<literal>rabbitmq</literal>："

msgid ""
"<literal>rabbitmq</literal> DRBD resource configuration (<filename>/etc/drbd."
"d/rabbitmq.res</filename>)"
msgstr ""
"<literal>rabbitmq</literal> DRBD 资源配置文件（ <filename>/etc/drbd.d/"
"rabbitmq.res</filename> ）"

msgid ""
"This resource uses an underlying local disk (in DRBD terminology, a backing "
"device) named <filename>/dev/data/rabbitmq</filename> on both cluster nodes, "
"<literal>node1</literal> and <literal>node2</literal>. Normally, this would "
"be an LVM Logical Volume specifically set aside for this purpose. The DRBD "
"meta-disk is internal, meaning DRBD-specific metadata is being stored at the "
"end of the disk device itself. The device is configured to communicate "
"between IPv4 addresses <literal>10.0.42.100</literal> and "
"<literal>10.0.42.254</literal>, using TCP port 7701. Once enabled, it will "
"map to a local DRBD block device with the device minor number 1, that is, "
"<filename>/dev/drbd1</filename>."
msgstr ""
"该资源使用了一块本地磁盘（DRBD 术语为“后端设备”， a backing device），该磁盘"
"在两台节点服务器（  <literal>node1</literal> ， <literal>node2</literal> ）上"
"对应相同的设备文件 ——<filename>/dev/data/rabbitmq</filename> ，一 般情况下，"
"该磁盘是一个专门为此配置的 LVM 逻辑卷。meta-disk 配置项的值是 internal，指的"
"是 DRBD 元数据保存在后端设备的结尾（即元数据和实际数据保存在同一存储设备"
"上）。设备数据同步通过 <literal>10.0.42.100</literal> 和 "
"<literal>10.0.42.254</literal> 完成，使用 TCP 7701 端口。当 DRBD 资源激活之"
"后，系统中将对应生成一个 DRBD 设备文件，次设备号为 1 ，设备文件是  "
"<filename>/dev/drbd1</filename> 。"

msgid ""
"Enabling a DRBD resource is explained in detail in <link href=\"http://www."
"drbd.org/users-guide-8.3/s-first-time-up.html\">the DRBD User's Guide</"
"link>. In brief, the proper sequence of commands is this:"
msgstr ""
"激活 DRBD 设备的详细说明请参阅 <link href=\"http://www.drbd.org/users-"
"guide-8.3/s-first-time-up.html\"> DRBD 用户手册 </link>，下面仅列出必要的命令"
"及其执行顺序："

msgid ""
"Initializes DRBD metadata and writes the initial set of metadata to "
"<filename>/dev/data/rabbitmq</filename>. Must be completed on both nodes."
msgstr ""
"初始化 DRBD 元数据，并在 <filename>/dev/data/rabbitmq</filename> 上初始元数据"
"集。两台节点服务器上都必须完成该操作。"

msgid ""
"Creates the <filename>/dev/drbd1</filename> device node, attaches the DRBD "
"device to its backing store, and connects the DRBD node to its peer. Must be "
"completed on both nodes."
msgstr ""
"创建 <literal>/dev/drbd1</literal> 设备文件，将指定的后端存储设备附加到该 "
"DRBD 资源，同时建立所有节点服务器之间的通信连接。两台节点服务器上都必须完成该"
"操作。"

msgid ""
"Kicks off the initial device synchronization, and puts the device into the "
"<literal>primary</literal> (readable and writable) role. See <link href="
"\"http://www.drbd.org/users-guide-8.3/ch-admin.html#s-roles\"> Resource "
"roles</link> (from the DRBD User's Guide) for a more detailed description of "
"the primary and secondary roles in DRBD. Must be completed on one node only, "
"namely the one where you are about to continue with creating your filesystem."
msgstr ""
"启动 DRBD 设备的初始化同步，并将之设置为 <literal>primary</literal> 角色。关"
"于 “primary” 和 “secondary” 角色的详细说明请查阅 <link href=\"http://www."
"drbd.org/users-guide-8.3/ch-admin.html#s-roles\"> Resource roles</link> "
"（ DRBD 用户手册）。这一操作只应在其中一台节点服务器上执行，此处指的是将被用"
"于执行创建文件系统的节点。"

msgid "Create a file system"
msgstr "创建文件系统"

msgid ""
"Once the DRBD resource is running and in the primary role (and potentially "
"still in the process of running the initial device synchronization), you may "
"proceed with creating the filesystem for RabbitMQ data. XFS is generally the "
"recommended filesystem:"
msgstr ""
"当 DRBD 资源已经激活并处于 “primay” 角色（可能初始化同步正在进行，还没有完"
"成），可以开始创建文件系统。 XFS 由于拥有日志系统，分配效率高，性能好等优点，"
"是建议选择的文件系统："

msgid ""
"You may also use the alternate device path for the DRBD device, which may be "
"easier to remember as it includes the self-explanatory resource name:"
msgstr "也可以使用 DRBD 设备的另外一个名称，该名称有解释含义，更容易记忆："

msgid ""
"Once completed, you may safely return the device to the secondary role. Any "
"ongoing device synchronization will continue in the background:"
msgstr ""
"完成后，可以安全地把设备变回 “secondary” 角色。已经启动的设备同步将在后台继续"
"进行："

msgid "Prepare RabbitMQ for Pacemaker high availability"
msgstr "RabbitMQ 针对 Pacemaker HA 架构的前期准备"

msgid ""
"In order for Pacemaker monitoring to function properly, you must ensure that "
"RabbitMQ's <filename>.erlang.cookie</filename> files are identical on all "
"nodes, regardless of whether DRBD is mounted there or not. The simplest way "
"of doing so is to take an existing <filename>.erlang.cookie</filename> from "
"one of your nodes, copying it to the RabbitMQ data directory on the other "
"node, and also copying it to the DRBD-backed filesystem."
msgstr ""
"要通过 Pacemaker 实现 RabbitMQ 高可用，必须首先保证 RabbitMQ 的文件 "
"<filename>.erlang.cookie</filename> 不论在 DRBD 设备有没有挂载的情况下，都完"
"全相同。最简单的方法是将其中一台节点服务器上已经生成的 <filename>.erlang."
"cookie</filename> 文件复制到所有其它节点，同时也复制一份到 DRBD 设备上的文件"
"系统之中。"

msgid "Add RabbitMQ resources to Pacemaker"
msgstr "在 Pacemaker 中添加 RabbitMQ 资源"

msgid ""
"You may now proceed with adding the Pacemaker configuration for RabbitMQ "
"resources. Connect to the Pacemaker cluster with <placeholder-1/>, and add "
"the following cluster resources:"
msgstr ""
"现在可以在 Pacemaker 中填加 RabbitMQ 相关资源。执行 <placeholder-1/> 命令进"
"入 Pacemaker 配置菜单，然后加入下列集群资源："

msgid ""
"<literal>p_ip_rabbitmq</literal>, a virtual IP address for use by RabbitMQ "
"(<literal>192.168.42.100</literal>),"
msgstr ""
"<literal>p_ip_rabbitmq</literal>，RabbitMQ 服务将会使用的虚拟 IP 地址"
"（<literal>192.168.42.100</literal>），"

msgid ""
"<literal>p_fs_rabbitmq</literal>, a Pacemaker managed filesystem mounted to "
"<filename>/var/lib/rabbitmq</filename> on whatever node currently runs the "
"RabbitMQ service,"
msgstr ""
"<literal>p_fs_rabbitmq</literal>，Pacemaker 管理的文件系统，挂载点为 "
"<filename>/var/lib/rabbitmq</filename>，该文件系统将在运行 RabbitMQ 服务的节"
"点上挂载，"

msgid ""
"<literal>ms_drbd_rabbitmq</literal>, the master/slave set managing the "
"<literal>rabbitmq</literal> DRBD resource,"
msgstr "<literal>ms_drbd_rabbitmq</literal>，管理 DRBD 设备的主/从资源，"

msgid ""
"a service group and order and colocation constraints to ensure resources are "
"started on the correct nodes, and in the correct sequence."
msgstr "资源组以及顺序、协同约束条件，会确保资源在正确的节点安照正确次序启动。"

msgid ""
"<placeholder-1/> supports batch input, so you may copy and paste the above "
"into your live pacemaker configuration, and then make changes as required. "
"For example, you may enter <literal>edit p_ip_rabbitmq</literal> from the "
"<placeholder-2/> menu and edit the resource to match your preferred virtual "
"IP address."
msgstr ""
"<placeholder-1/> 支持批量输入的配置，因此可以直接复制上述配置示例，粘贴到实际"
"的 Pacemaker 配置环境，然后根据具体情况调整配置项。例如，在  <placeholder-2/"
"> 菜单中，输入 <literal>edit p_ip_rabbitmq</literal>，可以对虚拟 IP 地址资源"
"进行编辑。"

msgid ""
"Once completed, commit your configuration changes by entering "
"<literal>commit</literal> from the <placeholder-1/> menu. Pacemaker will "
"then start the RabbitMQ service, and its dependent resources, on one of your "
"nodes."
msgstr ""
"配置完成后，在 <placeholder-1/> 菜单下输入 <literal>commit</literal> 提交所有"
"配置变更。随后 Pacemaker 会其中一台节点服务器上启动 RabbitMQ 服务（包括所有相"
"关资源）。"

msgid "Configure OpenStack services for highly available RabbitMQ"
msgstr "配置 OpenStack 各服务使用高可用的 RabbitMQ 服务"

msgid ""
"Your OpenStack services must now point their RabbitMQ configuration to the "
"highly available, virtual cluster IP addressrather than a RabbitMQ server's "
"physical IP address as you normally would."
msgstr ""
"现在可以将各 OpenStack 服务配置文件中使用物理 IP 地址的 RabbitMQ 访问方式，更"
"改为访问高可用、使用虚拟 IP 地址的 RabbitMQ 服务。"

msgid ""
"For OpenStack Image, for example, if your RabbitMQ service IP address is "
"<literal>192.168.42.100</literal> as in the configuration explained here, "
"you would use the following line in your OpenStack Image API configuration "
"file (<filename>glance-api.conf</filename>):"
msgstr ""
"以 OpenStack 镜像服务为例，如果 RabbitMQ 服务的虚拟 IP 地址是 "
"<literal>192.168.42.100</literal>，那么在 OpenStack 镜像服务的配置文件"
"（ <filename>glance-api.conf</filename> ）中应该使用如下配置："

msgid ""
"No other changes are necessary to your OpenStack configuration. If the node "
"currently hosting your RabbitMQ experiences a problem necessitating service "
"failover, your OpenStack services may experience a brief RabbitMQ "
"interruption, as they would in the event of a network hiccup, and then "
"continue to run normally."
msgstr ""
"除此之外，不需要更改其它配置。如果运行数据库服务的节点发生故障，RabbitMQ 服务"
"会自动迁移到其它节点，OpenStack 服务会经历短暂的临时 RabbitMQ 中断，和偶然发"
"生的网络中断类似，之后会继续正常运行。"

msgid "Highly available MySQL"
msgstr "高可用的MySQL"

msgid ""
"MySQL is the default database server used by many OpenStack services. Making "
"the MySQL service highly available involves:"
msgstr ""
"MySQL是许多OpenStack服务所使用的默认数据库服务。确保MySQL服务高可用涉及到："

msgid "Configuring a DRBD device for use by MySQL"
msgstr "配置被MySQL使用的DRBD设备"

msgid "Configuring MySQL to use a data directory residing on that DRBD device"
msgstr "配置MySQL使用位于DRBD设备上的数据目录"

msgid ""
"Selecting and assigning a virtual IP address (VIP) that can freely float "
"between cluster nodes"
msgstr "选择并绑定一个可以在各集群节点之间迁移的虚拟 IP 地址 (即 VIP )"

msgid "Configuring MySQL to listen on that IP address"
msgstr "配置 MySQL 监听那个 IP 地址"

msgid ""
"Managing all resources, including the MySQL daemon itself, with the "
"Pacemaker cluster manager"
msgstr "使用 Pacemaker 管理上述所有资源，包括 MySQL 数据库"

msgid ""
"<link href=\"http://galeracluster.com/\">MySQL/Galera</link> is an "
"alternative method of configuring MySQL for high availability. It is likely "
"to become the preferred method of achieving MySQL high availability once it "
"has sufficiently matured. At the time of writing, however, the Pacemaker/"
"DRBD based approach remains the recommended one for OpenStack environments."
msgstr ""
"<link href=\"http://galeracluster.com/\">MySQL/Galera</link> 是实现 MySQL 数"
"据库高可用的另一种可选方案，一旦该方案足够成熟，可能会是更好的选择。不过在本"
"文写作时，基于 Pacemaker/DRBD 的方案仍然是 OpenStack 环境的推荐方式。"

msgid ""
"The Pacemaker based MySQL server requires a DRBD resource from which it "
"mounts the <literal>/var/lib/mysql</literal> directory. In this example, the "
"DRBD resource is simply named <literal>mysql</literal>:"
msgstr ""
"基于 Pacemaker 的 MySQL 数据库需要一个 DRBD 设备，并将之挂载到 <literal>/var/"
"lib/mysql</literal> 目录。在示例中，DRBD 资源被简单命名为 <literal>mysql</"
"literal>："

msgid ""
"<literal>mysql</literal> DRBD resource configuration (<filename>/etc/drbd.d/"
"mysql.res</filename>)"
msgstr ""
"<literal>mysql</literal> DRBD 资源配置文件（ <filename>/etc/drbd.d/mysql."
"res</filename> ）"

msgid ""
"This resource uses an underlying local disk (in DRBD terminology, a backing "
"device) named <filename>/dev/data/mysql</filename> on both cluster nodes, "
"<literal>node1</literal> and <literal>node2</literal>. Normally, this would "
"be an LVM Logical Volume specifically set aside for this purpose. The DRBD "
"meta-disk is internal, meaning DRBD-specific metadata is being stored at the "
"end of the disk device itself. The device is configured to communicate "
"between IPv4 addresses <literal>10.0.42.100</literal> and "
"<literal>10.0.42.254</literal>, using TCP port 7700. Once enabled, it will "
"map to a local DRBD block device with the device minor number 0, that is, "
"<filename>/dev/drbd0</filename>."
msgstr ""
"该资源使用了一块本地磁盘（DRBD 术语为“后端设备”， a backing device），该磁盘"
"在两台节点服务器（  <literal>node1</literal> ， <literal>node2</literal> ）上"
"对应相同的设备文件 —— <filename>/dev/data/mysql</filename> ，一 般情况下，该"
"磁盘是一个专门为此配置的 LVM 逻辑卷。meta-disk 配置项的值是 internal，指的是 "
"DRBD 元数据保存在后端设备的结尾（即元数据和实际数据保存在同一存储设备上）。设"
"备数据同步通过 <literal>10.0.42.100</literal> 和 <literal>10.0.42.254</"
"literal> 完成，使用 TCP 7700 端口。当 DRBD 资源激活之后，系统中将对应生成一"
"个 DRBD 设备文件，次设备号为 0 ，设备文件是  <filename>/dev/drbd0</"
"filename> 。"

msgid ""
"Enabling a DRBD resource is explained in detail in <link href=\"http://www."
"drbd.org/users-guide-8.3/s-first-time-up.html\"> the DRBD User's Guide</"
"link>. In brief, the proper sequence of commands is this:"
msgstr ""
"激活 DRBD 设备的详细说明请参阅 <link href=\"http://www.drbd.org/users-"
"guide-8.3/s-first-time-up.html\"> DRBD 用户手册 </link>，下面仅列出必要的命令"
"及其执行顺序："

msgid ""
"Initializes DRBD metadata and writes the initial set of metadata to "
"<literal>/dev/data/mysql</literal>. Must be completed on both nodes."
msgstr ""
"初始化 DRBD 元数据，并在 <literal>/dev/data/mysql</literal> 上初始元数据集。"
"两台节点服务器上都必须完成该操作。"

msgid ""
"Creates the <literal>/dev/drbd0</literal> device node, attaches the DRBD "
"device to its backing store, and connects the DRBD node to its peer. Must be "
"completed on both nodes."
msgstr ""
"创建 <literal>/dev/drbd0</literal> 设备文件，将指定的后端存储设备附加到该 "
"DRBD 资源，同时建立所有节点服务器之间的通信连接。两台节点服务器上都必须完成该"
"操作。"

msgid "Creating a file system"
msgstr "创建文件系统"

msgid ""
"Once the DRBD resource is running and in the primary role (and potentially "
"still in the process of running the initial device synchronization), you may "
"proceed with creating the filesystem for MySQL data. XFS is generally the "
"recommended filesystem due to its journaling, efficient allocation, and "
"performance:"
msgstr ""
"当 DRBD 资源已经激活并处于 “primay” 角色(可能初始化同步正在进行，还没有完"
"成)，可以开始创建文件系统。 XFS 由于拥有日志系统，分配效率高，性能好等优点，"
"是建议选择的文件系统。"

msgid "Prepare MySQL for Pacemaker high availability"
msgstr "MySQL 针对 Pacemaker HA 架构的前期准备"

msgid ""
"In order for Pacemaker monitoring to function properly, you must ensure that "
"MySQL's database files reside on the DRBD device. If you already have an "
"existing MySQL database, the simplest approach is to just move the contents "
"of the existing <filename>/var/lib/mysql</filename> directory into the newly "
"created filesystem on the DRBD device."
msgstr ""
"要通过 Pacemaker 实现 MySQL 高可用，必须首先保证 MySQL 的数据文件使用 DRBD 存"
"储设备。如果使用已有的数据库，最简单的方法是将已有的数据库文件 <filename>/"
"var/lib/mysql</filename> 迁移到 DRBD 设备之上的文件系统中。"

msgid ""
"You must complete the next step while the MySQL database server is shut down."
msgstr "下面的步骤在必须关闭MySQL数据库服务器之后进行。"

msgid ""
"For a new MySQL installation with no existing data, you may also run the "
"<placeholder-1/> command:"
msgstr "如果使用全新的数据库，可以执行 <placeholder-1/> 命令："

msgid ""
"Regardless of the approach, the steps outlined here must be completed on "
"only one cluster node."
msgstr "这里列出的这些步骤只需要在其中一个集群节点上操作一遍即可。"

msgid "Add MySQL resources to Pacemaker"
msgstr "在 Pacemaker 中添加 MySQL 资源"

msgid ""
"You can now add the Pacemaker configuration for MySQL resources. Connect to "
"the Pacemaker cluster with <placeholder-1/>, and add the following cluster "
"resources:"
msgstr ""
"现在可以在 Pacemaker 中填加 MySQL 相关资源。执行 <placeholder-1/> 命令进入 "
"Pacemaker 配置菜单，然后加入下列集群资源："

msgid ""
"<literal>p_ip_mysql</literal>, a virtual IP address for use by MySQL "
"(<literal>192.168.42.101</literal>),"
msgstr ""
"<literal>p_ip_mysql</literal>，MySQL 服务将会使用的虚拟 IP 地址"
"（<literal>192.168.42.101</literal>），"

msgid ""
"<literal>p_fs_mysql</literal>, a Pacemaker managed filesystem mounted to "
"<filename>/var/lib/mysql</filename> on whatever node currently runs the "
"MySQL service,"
msgstr ""
"<literal>p_fs_mysql</literal>，Pacemaker 管理的文件系统，挂载点为 <filename>/"
"var/lib/mysql</filename>，该文件系统将在运行 MySQL 服务的节点上挂载，"

msgid ""
"<literal>ms_drbd_mysql</literal>, the master/slave set managing the "
"<literal>mysql</literal> DRBD resource,"
msgstr "<literal>ms_drbd_mysql</literal>，管理 DRBD 设备的主/从资源，"

msgid ""
"a service <literal>group</literal> and <literal>order</literal> and "
"<literal>colocation</literal> constraints to ensure resources are started on "
"the correct nodes, and in the correct sequence."
msgstr "资源组以及顺序、协同约束条件，会确保资源在正确的节点安照正确次序启动。"

msgid ""
"<placeholder-1/> supports batch input, so you may copy and paste the above "
"into your live pacemaker configuration, and then make changes as required. "
"For example, you may enter <literal>edit p_ip_mysql</literal> from the "
"<placeholder-2/> menu and edit the resource to match your preferred virtual "
"IP address."
msgstr ""
"<placeholder-1/> 支持批量输入的配置，因此可以直接复制上述配置示例，粘贴到实际"
"的 Pacemaker 配置环境，然后根据具体情况调整配置项。例如，在  <placeholder-2/"
"> 菜单中，输入 <literal>edit p_ip_mysql</literal>，可以对虚拟 IP 地址资源进行"
"编辑。"

msgid ""
"Once completed, commit your configuration changes by entering "
"<literal>commit</literal> from the <placeholder-1/> menu. Pacemaker will "
"then start the MySQL service, and its dependent resources, on one of your "
"nodes."
msgstr ""
"配置完成后，在 <placeholder-1/> 菜单下输入 <literal>commit</literal> 提交所有"
"配置变更。随后 Pacemaker 会其中一台节点服务器上启动 MySQL 服务（包括所有相关"
"资源）。"

msgid "Configure OpenStack services for highly available MySQL"
msgstr "配置 OpenStack 各服务使用高可用的 MySQL 数据库"

msgid ""
"Your OpenStack services must now point their MySQL configuration to the "
"highly available, virtual cluster IP addressrather than a MySQL server's "
"physical IP address as you normally would."
msgstr ""
"现在可以将各 OpenStack 服务配置文件中使用物理 IP 地址的 MySQL 访问方式，更改"
"为访问高可用、使用虚拟 IP 地址的 MySQL 服务。"

msgid ""
"For OpenStack Image, for example, if your MySQL service IP address is "
"<literal>192.168.42.101</literal> as in the configuration explained here, "
"you would use the following line in your OpenStack Image registry "
"configuration file (<filename>glance-registry.conf</filename>):"
msgstr ""
"以 OpenStack 镜像服务为例，如果 MySQL 数据库的虚拟 IP 地址是 "
"<literal>192.168.42.101</literal>，那么在 OpenStack 镜像服务的配置文件"
"（ <filename>glance-registry.conf</filename> ）中应该使用如下配置："

msgid ""
"No other changes are necessary to your OpenStack configuration. If the node "
"currently hosting your database experiences a problem necessitating service "
"failover, your OpenStack services may experience a brief MySQL interruption, "
"as they would in the event of a network hiccup, and then continue to run "
"normally."
msgstr ""
"除此之外，不需要更改其它配置。如果运行数据库服务的节点发生故障，MySQL 服务会"
"自动迁移到其它节点，OpenStack 服务会经历短暂的临时 MySQL 中断，和偶然发生的网"
"络中断类似，之后会继续正常运行。"

msgid "Memcached"
msgstr "Memcached"

msgid ""
"Most OpenStack services use an application to offer persistence and store "
"ephemeral data like tokens. <application>Memcached</application> is one of "
"them and can scale-out easily without any specific tricks required."
msgstr ""
"大部分 OpenStack 服务都会使用另外一个应用程序来提供持久化存储并存储类似认证令"
"牌的附加数据。<application>Memcached</application> 是其中一种，而且无需特殊配"
"置就可以分布式地运行在多个节点服务器上。"

msgid ""
"To install and configure it, read the <link href=\"http://code.google.com/p/"
"memcached/wiki/NewStart\"> official documentation</link>."
msgstr ""
"<application>Memcached</application> 的安装和配置，请参阅 <link href="
"\"http://code.google.com/p/memcached/wiki/NewStart\"> 官方文档</link>。"

msgid ""
"Memory caching is managed by oslo-incubator, so the way to use multiple "
"memcached servers is the same for all projects."
msgstr ""
"基于内存的缓存统一由 oslo-incubator 管理，因此对 OpenStack 服务来说，使用多"
"个 memcached 服务节点作为后端的方法完全相同。"

msgid "Example configuration with two hosts:"
msgstr "使用 2 个 memcached 节点的配置示例："

msgid ""
"By default, <literal>controller1</literal> handles the caching service but "
"if the host goes down, <literal>controller2</literal> does the job. For more "
"information about <application>Memcached</application> installation, see the "
"<link href=\"http://docs.openstack.org/admin-guide-cloud/content/\"> "
"OpenStack Cloud Administrator Guide</link>."
msgstr ""
"默认情况下，服务器 <literal>controller1</literal> 处理所有缓存，一旦它无法工"
"作，服务器 <literal>controller2</literal> 会自动接管。更多关于 "
"<application>Memcached</application> 布署的信息请参阅  <link href=\"http://"
"docs.openstack.org/admin-guide-cloud/content/\"> OpenStack 云计算平台管理员手"
"册</link>。"

msgid "Run OpenStack API and schedulers"
msgstr "运行 OpenStack API 和 调度服务"

msgid "API services"
msgstr "API 服务"

msgid ""
"To use highly available and scalable API services, we need to ensure that:"
msgstr "要实现高可用和可扩展的 API 服务，需要保证："

msgid ""
"You use virtual IP addresses when configuring OpenStack Identity endpoints."
msgstr "为 OpenStack 身份认证中的服务端点配置虚拟 IP 地址。"

msgid "All OpenStack configuration files should refer to virtual IP addresses."
msgstr "所有 OpenStack 组件的配置中也相应使用虚拟 IP 地址。"

msgid "Schedulers"
msgstr "调度程序"

msgid "nova-scheduler"
msgstr "nova-scheduler"

msgid "nova-conductor"
msgstr "nova-conductor"

msgid "cinder-scheduler"
msgstr "cinder-scheduler"

msgid "neutron-server"
msgstr "neutron-server"

msgid "ceilometer-collector"
msgstr "ceilometer-collector"

msgid "heat-engine"
msgstr "heat-engine"

msgid ""
"Please refer to the <link linkend=\"s-rabbitmq\">RabbitMQ section</link> for "
"configuring these services with multiple messaging servers."
msgstr ""
"请参阅  <link linkend=\"s-rabbitmq\">RabbitMQ 部分</link> 将 OpenStack 各组件"
"配置为可同时使用多个消息队列服务器。"

msgid "Start Pacemaker"
msgstr "启动 Pacemaker"

msgid ""
"Once the Corosync services have been started and you have established that "
"the cluster is communicating properly, it is safe to start <systemitem class="
"\"service\">pacemakerd</systemitem>, the Pacemaker master control process:"
msgstr ""
"Corosync 服务启动之后，一旦各节点正常建立集群通信，就可启动 <systemitem "
"class=\"service\">pacemakerd</systemitem> （ Pacemaker 主进程）："

msgid "<placeholder-1/> (LSB)"
msgstr "<placeholder-1/> （LSB）"

msgid "<placeholder-1/> (LSB, alternate)"
msgstr "<placeholder-1/> （LSB，另一种方法）"

msgid "<placeholder-1/> (upstart)"
msgstr "<placeholder-1/> （upstart）"

msgid "<placeholder-1/> (systemd)"
msgstr "<placeholder-1/> （systemd）"

msgid ""
"Once the Pacemaker services have started, Pacemaker will create a default "
"empty cluster configuration with no resources. You may observe Pacemaker's "
"status with the <placeholder-1/> utility:"
msgstr ""
"Pacemaker 服务启动之后，会自动建立一份空白的集群配置，不包含任何资源。可以通"
"过 <placeholder-1/> 工具查看 Packemaker 集群的状态："

msgid "Set up Corosync"
msgstr "Corosync 基本配置"

msgid ""
"Besides installing the <package>Corosync</package> package, you must also "
"create a configuration file, stored in <filename>/etc/corosync/corosync."
"conf</filename>. Corosync can be configured to work with either multicast or "
"unicast IP addresses."
msgstr ""
"除了安装 <package>Corosync</package> 之外，还需要创建一个配置文件 <filename>/"
"etc/corosync/corosync.conf</filename> 。Corosync 可以使用组播或者单播 IP 地址"
"进行集群心跳通信。"

msgid "Set up Corosync with multicast"
msgstr "配置 Corosync 使用组播"

msgid ""
"Most distributions ship an example configuration file (<filename>corosync."
"conf.example</filename>) as part of the documentation bundled with the "
"<package>Corosync</package> package. An example Corosync configuration file "
"is shown below:"
msgstr ""
"大多数 Linux 发行版都会中在 <package>Corosync</package> 软件包中附带一份配置"
"示例(<filename>corosync.conf.example</filename>)。Corosync示例配置文件如下："

msgid "Corosync configuration file (<filename>corosync.conf</filename>)"
msgstr "Corosync 配置文件（<filename>corosync.conf</filename>）"

msgid ""
"The <placeholder-1/> value specifies the time, in milliseconds, during which "
"the Corosync token is expected to be transmitted around the ring. When this "
"timeout expires, the token is declared lost, and after <placeholder-2/> lost "
"tokens the non-responding processor (cluster node) is declared dead. In "
"other words, <placeholder-3/> × <placeholder-4/> is the maximum time a node "
"is allowed to not respond to cluster messages before being considered dead. "
"The default for <placeholder-5/> is 1000 (1 second), with 4 allowed "
"retransmits. These defaults are intended to minimize failover times, but can "
"cause frequent \"false alarms\" and unintended failovers in case of short "
"network interruptions. The values used here are safer, albeit with slightly "
"extended failover times."
msgstr ""
"<placeholder-1/> 是时间，单位为毫秒，在该配置项指定的时间内， Corosync 令牌应"
"该完成在回环网络中的传输。如果令牌传输超时就会被丢弃，而一台节点服务器连续出"
"现  <placeholder-2/> 令牌失效，将会被认为是无效节点。也就是说，一台节点服务器"
"最长的无响应时间不能超对 <placeholder-3/> × <placeholder-4/>  的乘积（单位毫"
"秒），否则会被认为是无效节点。<placeholder-5/> 的默认值是 1000 （即 1 秒），"
"同时默认的重试次数为 4 。默认配置的目标是尽量缩短故障恢复时间，但是可能出现较"
"多的 “false alarm” 提醒，发生短期的网络故障时也有可能导致失效切换。本处示例中"
"的配置参数更安全一些，但是失效切换的时间会长一些。"

msgid ""
"With <placeholder-1/> enabled, Corosync nodes mutually authenticate using a "
"128-byte shared secret stored in <filename>/etc/corosync/authkey</filename>, "
"which may be generated with the <placeholder-2/> utility. When using "
"<placeholder-3/>, cluster communications are also encrypted."
msgstr ""
"当启用 <placeholder-1/> 时，Corosync 节点之间通信时会使用一个 128 位的密钥进"
"行双向认证。密钥存放在 <filename>/etc/corosync/authkey</filename> 文件中，可"
"以通过 <placeholder-2/> 命令生成。启用 <placeholder-3/> 后，集群通信数据也会"
"进行加密。"

msgid ""
"In Corosync configurations using redundant networking (with more than one "
"<placeholder-1/>), you must select a Redundant Ring Protocol (RRP) mode "
"other than <literal>none</literal>. <literal>active</literal> is the "
"recommended RRP mode."
msgstr ""
"Cororsync 可以使用冗余的心跳网络（即多个 <placeholder-1/> 配置），但是必须同"
"时将 RRP 模式设置为除 <literal>none</literal>之外的其它值，建议使用 "
"<literal>active</literal> 模式。"

msgid ""
"There are several things to note about the recommended interface "
"configuration:"
msgstr "在推荐的网络接口配置中有几件事需要注意："

msgid ""
"The <placeholder-1/> must differ between all configured interfaces, starting "
"with 0."
msgstr "所有心跳网络的 <placeholder-1/> 配置不能重复，最小值为 0 。"

msgid ""
"The <placeholder-1/> is the network address of the interfaces to bind to. "
"The example uses two network addresses of <literal>/24</literal> IPv4 "
"subnets."
msgstr ""
"<placeholder-1/> 是心跳网卡 IP 地址对应的网络地址。示例中使用了两个子网掩码"
"为 <literal>/24</literal> 的 IPv4 网段。"

msgid ""
"Multicast groups (<placeholder-1/>) must not be reused across cluster "
"boundaries. In other words, no two distinct clusters should ever use the "
"same multicast group. Be sure to select multicast addresses compliant with "
"<link href=\"http://www.ietf.org/rfc/rfc2365.txt\">RFC 2365, "
"\"Administratively Scoped IP Multicast\"</link>."
msgstr ""
"多播地址的使用（<placeholder-1/> ） 不要跨越集群的边界，也即是说，不要在不同"
"的集群中使用相同的多播地址。使用多播地址时，请依据 <link href=\"http://www."
"ietf.org/rfc/rfc2365.txt\">RFC 2365, \"Administratively Scoped IP Multicast"
"\"</link> 。"

msgid ""
"For firewall configurations, note that Corosync communicates over UDP only, "
"and uses <literal>mcastport</literal> (for receives) and <literal>mcastport "
"- 1</literal> (for sends)."
msgstr ""
"Corosync 通信使用 UDP 协议，端口为 <literal>mcastport</literal> （接收数据）"
"和 <literal>mcastport - 1</literal> （发送数据）。配置防火墙时需要打开这两个"
"端口。"

msgid ""
"The <literal>service</literal> declaration for the <literal>pacemaker</"
"literal> service may be placed in the <filename>corosync.conf</filename> "
"file directly, or in its own separate file, <filename>/etc/corosync/service."
"d/pacemaker</filename>."
msgstr ""
"<literal>pacemaker</literal> 对应的 <literal>service</literal> 配置段，可以放"
"在 <filename>corosync.conf</filename> ，也可以单独作为一个配置文件 "
"<filename>/etc/corosync/service.d/pacemaker</filename> 。"

msgid ""
"If you are using Corosync version 2 on Ubuntu 14.04, remove or comment out "
"lines under the service stanza, which enables Pacemaker to start up."
msgstr ""
"如果是在 Ubuntu 14.04 系统中运行 Corosync 2，那么应该将 stanza 对应的 "
"service 配置段删除或者全部注释，以确保 Pacemaker 可以启动。"

msgid ""
"Once created, the <filename>corosync.conf</filename> file (and the "
"<filename>authkey</filename> file if the <placeholder-1/> option is enabled) "
"must be synchronized across all cluster nodes."
msgstr ""
"<filename>corosync.conf</filename> （以及 <filename>authkey</filename> ，如"
"果 <placeholder-1/> 启用）一旦创建，则必须在各节点服务器之间保持同步。"

msgid "Set up Corosync with unicast"
msgstr "配置 Corosync 使用单播"

msgid ""
"Some environments may not support multicast. For such cases, Corosync should "
"be configured for unicast. An example fragment of the Corosync configuration "
"file is shown below:"
msgstr ""
"某些环境中可能不支持组播。这时应该配置 Corosync 使用单播，下面是使用单播的 "
"Corosync 配置文件的一部分："

msgid ""
"Corosync configuration file fragment (<filename>corosync.conf</filename>)"
msgstr ""
"Corosync 配置文件片断：\n"
"（ <filename>corosync.conf</filename> ）"

msgid ""
"If the <placeholder-1/> is set to yes, the broadcast address is used for "
"communication. If this option is set, <placeholder-2/> should not be set."
msgstr ""
"如果将 <placeholder-1/>  设置为 yes ，集群心跳将通过广播实现。设置该参数时，"
"不能设置 <placeholder-2/> 。"

msgid ""
"The <placeholder-1/> directive controls the transport mechanism used. To "
"avoid the use of multicast entirely, a unicast transport parameter "
"<placeholder-2/> should be specified. This requires specifying the list of "
"members in <placeholder-3/> directive; this could potentially make up the "
"membership before deployment. The default is <placeholder-4/>. The transport "
"type can also be set to <placeholder-5/> or <placeholder-6/>."
msgstr ""
"<placeholder-1/> 配置项决定集群通信方式。要完全禁用组播，应该配置单播传输参"
"数 <placeholder-2/> 。这要求将所有的节点服务器信息写入 <placeholder-3/> ，也"
"就是需要在配署 HA 集群之前确定节点组成。配认配置是 <placeholder-4/> 。通信方"
"式类型还支持 <placeholder-5/> 和 <placeholder-6/> 。"

msgid ""
"Within the <placeholder-1/> directive, it is possible to specify specific "
"information about nodes in cluster. Directive can contain only the "
"<placeholder-2/> sub-directive, which specifies every node that should be a "
"member of the membership, and where non-default options are needed. Every "
"node must have at least the <placeholder-3/> field filled."
msgstr ""
"在 <placeholder-1/> 之下可以为某一节点设置只与该节点相关的信息，这些设置项只"
"能包含在 <placeholder-2/> 之中，即只能对属于集群的节点服务器进行设置，而且只"
"应包括那些与默认设置不同的参数。每台服务器都必须配置 <placeholder-3/> 。"

msgid ""
"For UDPU, every node that should be a member of the membership must be "
"specified."
msgstr "对于 UDPU ，每台节点服务器都需要配置所属的传输组。"

msgid "Possible options are:"
msgstr "可用的节点服务器配置项有："

msgid ""
"The <placeholder-1/> specifies IP address of one of the nodes. X is ring "
"number."
msgstr ""
"<placeholder-1/> 对应该节点服务器用于集群通信的 IP 地址，其中 X 对应集群通信"
"环路序号。"

msgid ""
"The <placeholder-1/> configuration option is optional when using IPv4 and "
"required when using IPv6. This is a 32-bit value specifying the node "
"identifier delivered to the cluster membership service. If this is not "
"specified with IPv4, the node id will be determined from the 32-bit IP "
"address the system to which the system is bound with ring identifier of 0. "
"The node identifier value of zero is reserved and should not be used."
msgstr ""
"<placeholder-1/> 在使用 IPv4 的网络环境中是可选配置，但在 IPv6 的网络环境中则"
"必须进行配置。它是一个 32 位的数字，用于在整个集群中标识一台节点服务器。在 "
"IPv4 网络环境中如果不进行配置，节点 ID 将根据 ring0 对应的 IP 地址自动生成。"
"节点 ID 0 是保留值，不能在配置文件中使用。"

msgid "Install packages"
msgstr "安装软件包"

msgid ""
"On any host that is meant to be part of a Pacemaker cluster, you must first "
"establish cluster communications through the Corosync messaging layer. This "
"involves installing the following packages (and their dependencies, which "
"your package manager will normally install automatically):"
msgstr ""
"Pacemaker 中的节点服务器之间必须通过 Corosync 建立集群通信，需要安装以下软件"
"包(以及它们的依赖软件包，通常软件包管理器将自动所有依赖软件包)："

msgid ""
"<package>pacemaker</package> (Note that the crm shell should be downloaded "
"separately.)"
msgstr ""
"<package>pacemaker</package> （说明：crm 命令行工具需要另外单独下载。）"

msgid "crmsh"
msgstr "crmsh"

msgid "corosync"
msgstr "corosync"

msgid "cluster-glue"
msgstr "cluster-glue"

msgid ""
"<package>fence-agents</package> (Fedora only; all other distributions use "
"fencing agents from <package>cluster-glue</package>)"
msgstr ""
"<package>fence-agents</package> （说明：只针对 Fedora 发行版；其它 Linux 发行"
"版都使用 <package>cluster-glue</package> 软件包中的 fence 资源代理）"

msgid "resource-agents"
msgstr "resource-agents"

msgid "Set basic cluster properties"
msgstr "设置集群基本属性"

msgid ""
"Once your Pacemaker cluster is set up, it is recommended to set a few basic "
"cluster properties. To do so, start the <placeholder-1/> shell and change "
"into the configuration menu by entering <literal>configure</literal>. "
"Alternatively, you may jump straight into the Pacemaker configuration menu "
"by typing <placeholder-2/> directly from a shell prompt."
msgstr ""
"Pacemaker 启动之后，建议首先对集群基本属性进行配置。配置时，首先执行 "
"<placeholder-1/> 命令，然后输入 <literal>configure</literal> 进入配置菜单。也"
"可以执行 <placeholder-2/> 命令直接进入 Pacemaker 配置菜单。"

msgid "Then, set the following properties:"
msgstr "然后，设置下列属性："

msgid ""
"Setting <placeholder-1/> is required in 2-node Pacemaker clusters for the "
"following reason: if quorum enforcement is enabled, and one of the two nodes "
"fails, then the remaining node can not establish a majority of quorum votes "
"necessary to run services, and thus it is unable to take over any resources. "
"In this case, the appropriate workaround is to ignore loss of quorum in the "
"cluster. This should only only be done in 2-node clusters: do not set this "
"property in Pacemaker clusters with more than two nodes. Note that a two-"
"node cluster with this setting exposes a risk of split-brain because either "
"half of the cluster, or both, are able to become active in the event that "
"both nodes remain online but lose communication with one another. The "
"preferred configuration is 3 or more nodes per cluster."
msgstr ""
"对于 2 个节点的 Pacemaker 集群，集群属性 <placeholder-1/> 是必须配置的，因"
"为：如果强制集群满足合法节点数要求，当 其中一个节点失效时，剩下的一个节点无法"
"达到集群多数节点在线的要求，从而不能接管原来运行在失效节点上的集群资源。这种"
"情况下，解决方法只能是忽略集群合法节点数要求。但是这一属性只能用于 2 个节点的"
"集群，对于 3 个节点及以上的集群来说，是不应该配置该属性的。需要注意的是，2 个"
"节点的集群配置该属性之后，会出现脑裂（ split-brain）的风险，这是因为当 2 个节"
"点都在线但是互相无法通信时，2 个节点都认为对方出现故障，从而尝试接管对方的集"
"群资源。因此建议布署 3 个节点及以上的集群。"

msgid ""
"Setting <placeholder-1/>, <placeholder-2/> and <placeholder-3/> to 1000 "
"instructs Pacemaker to keep a longer history of the inputs processed, and "
"errors and warnings generated, by its Policy Engine. This history is "
"typically useful in case cluster troubleshooting becomes necessary."
msgstr ""
"将 <placeholder-1/> 、 <placeholder-2/> 以及 <placeholder-3/> 设置为 1000，是"
"为了让 Pacemaker 保存更多 Policy Engine 的处理输入、错误以及警告信息。这些历"
"史记录对排除集群故障会有很大帮忙。"

msgid ""
"Pacemaker uses an event-driven approach to cluster state processing. "
"However, certain Pacemaker actions occur at a configurable interval, "
"<placeholder-1/>, which defaults to 15 minutes. It is usually prudent to "
"reduce this to a shorter interval, such as 5 or 3 minutes."
msgstr ""
"Pacemaker 处理集群状况时使用事件驱动机制。但是某些 Pacemaker 操作只会在固定的"
"时间间隔触发。该时间间隔可以配置，<placeholder-1/>，默认值是 15 分钟。针对特"
"定的集群，可以适当缩短这一间隔，如 5 分钟或者 3 分钟。"

msgid ""
"Once you have made these changes, you may <literal>commit</literal> the "
"updated configuration."
msgstr "作完这些改变后，可以提交更新配置。"

msgid "Starting Corosync"
msgstr "启动Corosync"

msgid ""
"Corosync is started as a regular system service. Depending on your "
"distribution, it may ship with an LSB init script, an upstart job, or a "
"systemd unit file. Either way, the service is usually named <systemitem "
"class=\"service\">corosync</systemitem>:"
msgstr ""
"Corosync 启动方法和普通的系统服务没有区别，根据 Linux 发行版的不同，可能是 "
"LSB init 脚本、upstart 任务、systemd 服务。不过习惯上，都会统一使用 "
"<systemitem class=\"service\">corosync</systemitem> 这一名称："

msgid "You can now check the Corosync connectivity with two tools."
msgstr "使用以下两个工具检查 Corosync 连接状态。"

msgid ""
"The <placeholder-1/> utility, when invoked with the <placeholder-2/> option, "
"gives a summary of the health of the communication rings:"
msgstr ""
"<placeholder-1/> ，执行时加上  <placeholder-2/> 参数，可以获取整个集群通信的"
"健康情况："

msgid ""
"The <placeholder-1/> utility can be used to dump the Corosync cluster member "
"list:"
msgstr "<placeholder-1/> 命令可以列出 Corosync 集群的成员节点列表："

msgid ""
"You should see a <literal>status=joined</literal> entry for each of your "
"constituent cluster nodes."
msgstr "status=joined标示着每一个集群节点成员。"

msgid ""
"If you are using Corosync version 2, use the <placeholder-1/> utility as it "
"is a direct replacement for <placeholder-2/>."
msgstr ""
"如果使用 Corosync v2 版本，请使用 <placeholder-2/> 命令的替代命令 "
"<placeholder-1/>  。"

msgid "Configure OpenStack services to use RabbitMQ"
msgstr "配置 OpenStack 各服务使用高可用的 RabbitMQ 服务"

msgid ""
"We have to configure the OpenStack components to use at least two RabbitMQ "
"nodes."
msgstr ""
"现在可以配置 OpenStack 其它组件使用高可用 RabbitMQ 集群（最少使用其中 2 台节"
"点服务器）。"

msgid "Do this configuration on all services using RabbitMQ:"
msgstr "对所有使用 RabbitMQ 的组件进行配置："

msgid "RabbitMQ HA cluster host:port pairs:"
msgstr "RabbitMQ HA 集群服务地址及端口："

msgid "How frequently to retry connecting with RabbitMQ:"
msgstr "重新尝试连接 RabbitMQ 服务的时间间隔："

msgid "How long to back-off for between retries when connecting to RabbitMQ:"
msgstr "每次重新尝试连接 RabbitMQ 服务应后延多长时间："

msgid ""
"Maximum retries with trying to connect to RabbitMQ (infinite by default):"
msgstr "连接 RabbitMQ 服务时最大的重试次数（默认没有限制）："

msgid "Use durable queues in RabbitMQ:"
msgstr "是否使用持久的消息队列："

msgid "Use HA queues in RabbitMQ (x-ha-policy: all):"
msgstr "否使用 RabbitMMQ 的队列镜像特性（ x-ha-policy: all ）："

msgid ""
"If you change the configuration from an old setup which did not use HA "
"queues, you should interrupt the service:"
msgstr ""
"如果是直接俢改没有启用队列镜像特性的 RabbitMQ 服务的配置，那么对服务作一次重"
"置："

msgid "Services currently working with HA queues:"
msgstr "目前支持高可用 RabbitMQ 服务的 OpenStack 组件有："

msgid "OpenStack Compute"
msgstr "OpenStack 计算服务"

msgid "OpenStack Block Storage"
msgstr "OpenStack 块设备存储服务"

msgid "OpenStack Networking"
msgstr "OpenStack 网络服务"

msgid "Telemetry"
msgstr "Telemetry"

msgid "On Ubuntu and Debian"
msgstr "对于 Ubuntu 和 Debian 发行版"

msgid "RabbitMQ is packaged on both distros:"
msgstr "RabbitMQ 已经有可用的软件安装包："

msgid "Official manual for installing RabbitMQ on Ubuntu and Debian"
msgstr "在 Ubuntu 和 Debian 发行版上安装 RabbitMQ 的官方文档"

msgid "On Fedora and RHEL"
msgstr "对于 Fedora 和 RHEL 发行版"

msgid "Official manual for installing RabbitMQ on Fedora and RHEL"
msgstr "在 Fedora 和 RHEL 发行版上安装 RabbitMQ 的官方文档"

msgid "On openSUSE and SLES"
msgstr "对 openSUSE 和 SLES 发行版"

msgid "On openSUSE:"
msgstr "在 openSUSE 系统中："

msgid "Official manual for installing RabbitMQ on openSUSE"
msgstr "在 openSUSE 发行版上安装 RabbitMQ 的官方文档"

msgid ""
"The packages are signed by GPG key 893A90DAD85F9316. You should verify the "
"fingerprint of the imported GPG key before using it."
msgstr ""
"这些软件包都使用 GPG 密钥 893A90DAD85F9316 进行了签名，在安装之前可以先验证签"
"名。"

msgid "Configure RabbitMQ"
msgstr "配置 RabbitMQ"

msgid ""
"We are building a cluster of RabbitMQ nodes to construct a RabbitMQ broker, "
"a logical grouping of several Erlang nodes."
msgstr ""
"将多个 RabbitMQ 节点组织成一个集群，构建一个 RabbitMQ broker 服务，即一个 "
"Erlang 节点的逻辑集合。"

msgid ""
"We have to consider that while exchanges and bindings will survive the loss "
"of individual nodes, queues and their messages will not because a queue and "
"its contents is located on one node. If we lose this node, we also lose the "
"queue."
msgstr ""
"单个节点服务器的故障不会导致消息的交换和绑定完全不可用，但是具体一个消息队列"
"及其中的内容则相反。原因是消息队列和其中的内容只在其中一台节点上，该节点出现"
"故障无法工作时，整个消息队列就丢失了。"

msgid ""
"Mirrored queues in RabbitMQ improve the availability of service since it "
"will be resilient to failures."
msgstr "RabbitMQ 实现队列镜像更能提高整个集群的高可用性。"

msgid ""
"We consider that we run (at least) two RabbitMQ servers and we call the "
"nodes <literal>rabbit1</literal> and <literal>rabbit2</literal>. To build a "
"broker, we need to ensure that all nodes have the same Erlang cookie file."
msgstr ""
"示例中会布署 2 台 RabbitMQ 服务器，<literal>rabbit1</literal> 和 "
"<literal>rabbit2</literal>。要构建一个 RabbitMQ broker 服务，必须保证所有节点"
"服务器的 Erlang cookie 文件完全相同。"

msgid ""
"To do so, stop RabbitMQ everywhere and copy the cookie from the first node "
"to the other node(s):"
msgstr ""
"因此，首先在所有节点服务器停止 RabbitMQ 服务，然后将第一台节点服务上的 "
"cookis 文件复制到其它节点："

msgid "NODE"
msgstr "NODE"

msgid "To verify the cluster status:"
msgstr "检查集群状态："

msgid ""
"If the cluster is working, you can now proceed to creating users and "
"passwords for queues."
msgstr "如果集群运行正常，就可以开始为消息队列创建用户和密码。"

msgid ""
"More information about <link href=\"http://www.rabbitmq.com/ha.html\"> "
"highly available queues</link> and <link href=\"https://www.rabbitmq.com/"
"clustering.html\"> clustering</link> can be found in the official RabbitMQ "
"documentation."
msgstr ""
"关于 RabbitMQ 的高可用配置，请参阅 RabbitMQ 的官方文档：<link href=\"http://"
"www.rabbitmq.com/ha.html\"> highly available queues</link> and <link href="
"\"https://www.rabbitmq.com/clustering.html\"> clustering</link>。"

msgid "Highly available neutron metadata agent"
msgstr "高可用 neutron metadata 代理程序"

msgid ""
"Neutron metadata agent allows Compute API metadata to be reachable by VMs on "
"tenant networks. High availability for the metadata agent is achieved by "
"adopting Pacemaker."
msgstr ""
"Neutron metadata 代理程序的作用是让运行在租户网络上的虚拟机实例能够访问 "
"OpenStack 计算服务 API 元数据。Neutron metadata 代理程序的高可用也通过 "
"Pacemaker 实现。"

msgid "Add neutron metadata agent resource to Pacemaker"
msgstr "在 Pacemaker 中添加 neutron metadata 代理程序资源"

msgid ""
"You may now proceed with adding the Pacemaker configuration for neutron "
"metadata agent resource. Connect to the Pacemaker cluster with <literal>crm "
"configure</literal>, and add the following cluster resources:"
msgstr ""
"现在可以在 Pacemaker 中填加 neutron metadata 代理程序相关资源。执行 "
"<literal>crm configure</literal> 命令进入 Pacemaker 配置菜单，然后加入下列集"
"群资源："

msgid ""
"<literal>p_neutron-metadata-agent</literal>, a resource for manage Neutron "
"Metadata Agent service"
msgstr ""
"<literal>p_neutron-metadata-agent</literal>资源，对 neutron metadata 代理程序"
"进行管理。"

msgid ""
"<literal>crm configure</literal> supports batch input, so you may copy and "
"paste the above into your live Pacemaker configuration, and then make "
"changes as required."
msgstr ""
"<literal>crm configure</literal> 支持批量输入，因此可以拷贝粘贴上面到现有的 "
"Pacemaker 配置中，然后根据需要再作修改。"

msgid ""
"Once completed, commit your configuration changes by entering "
"<literal>commit</literal> from the <literal>crm configure</literal> menu. "
"Pacemaker will then start the neutron metadata agent service, and its "
"dependent resources, on one of your nodes."
msgstr ""
"配置完成后，在 <literal>crm configure</literal>  菜单下输入 <literal>commit</"
"literal> 提交所有配置变更。随后 Pacemaker 会其中一台节点服务器上启动 neutron "
"metadata 代理程序（包括所有相关资源）。"

msgid "Highly available neutron L3 agent"
msgstr "高可用 neutron L3 代理程序"

msgid ""
"The neutron L3 agent provides L3/NAT forwarding to ensure external network "
"access for VMs on tenant networks. High availability for the L3 agent is "
"achieved by adopting Pacemaker."
msgstr ""
"Neutron L3 代理程序负责实现 L3/NAT 转发，让运行在租户网络上的虚拟机实例能够访"
"问外部网络。Neutron L3 代理程序实现高可用也基于 Pacemaker 。"

msgid "Add neutron L3 agent resource to Pacemaker"
msgstr "在 Pacemaker 中添加 neutron L3 代理程序资源"

msgid ""
"You may now proceed with adding the Pacemaker configuration for neutron L3 "
"agent resource. Connect to the Pacemaker cluster with <literal>crm "
"configure</literal>, and add the following cluster resources:"
msgstr ""
"现在可以在 Pacemaker 中填加 neutron L3 代理程序相关资源。执行 <literal>crm "
"configure</literal> 命令进入 Pacemaker 配置菜单，然后加入下列集群资源："

msgid ""
"<literal>p_neutron-l3-agent</literal>, a resource for manage Neutron L3 "
"Agent service"
msgstr ""
"<literal>p_neutron-l3-agent</literal>资源，对 neutron L3 代理程序进行管理。"

msgid ""
"Once completed, commit your configuration changes by entering "
"<literal>commit</literal> from the <literal>crm configure</literal> menu. "
"Pacemaker will then start the neutron L3 agent service, and its dependent "
"resources, on one of your nodes."
msgstr ""
"配置完成后，在 <literal>crm configure</literal>  菜单下输入 <literal>commit</"
"literal> 提交所有配置变更。随后 Pacemaker 会其中一台节点服务器上启动 neutron "
"L3 代理程序（包括所有相关资源）。"

msgid ""
"This method does not ensure a zero downtime since it has to recreate all the "
"namespaces and virtual routers on the node."
msgstr ""
"这种高可用方案不能实现“零停机”需求，原因是neutron L3 代理程序切换时需要重新创"
"建网络命名空间和虚拟路由器。"

msgid "Manage network resources"
msgstr "组织网络相关资源"

msgid ""
"You can now add the Pacemaker configuration for managing all network "
"resources together with a group. Connect to the Pacemaker cluster with "
"<literal>crm configure</literal>, and add the following cluster resources:"
msgstr ""
"创建一个资源组将所有网络服务相关资源联系起来。执行 <literal>crm configure</"
"literal> 命令进入 Pacemaker 配置菜单，然后加入下列集群资源："

msgid "Highly available neutron DHCP agent"
msgstr "高可用 neutron DHCP 代理程序"

msgid "Add neutron DHCP agent resource to Pacemaker"
msgstr "在 Pacemaker 中添加 neutron DHCP 代理程序资源"

msgid ""
"You may now proceed with adding the Pacemaker configuration for neutron DHCP "
"agent resource. Connect to the Pacemaker cluster with <literal>crm "
"configure</literal>, and add the following cluster resources:"
msgstr ""
"现在可以在 Pacemaker 中填加 neutron DHCP 代理程序相关资源。执行 <literal>crm "
"configure</literal> 命令进入 Pacemaker 配置菜单，然后加入下列集群资源："

msgid "This configuration creates:"
msgstr "该配置会创建："

msgid ""
"Once completed, commit your configuration changes by entering "
"<literal>commit</literal> from the <literal>crm configure</literal> menu. "
"Pacemaker will then start the neutron DHCP agent service, and its dependent "
"resources, on one of your nodes."
msgstr ""
"配置完成后，在 <literal>crm configure</literal>  菜单下输入 <literal>commit</"
"literal> 提交所有配置变更。随后 Pacemaker 会其中一台节点服务器上启动 neutron "
"DHCP 代理程序（包括所有相关资源）。"

#. Put one translator per line, in the form of NAME <EMAIL>, YEAR1, YEAR2
msgid "translator-credits"
msgstr "translator-credits"
